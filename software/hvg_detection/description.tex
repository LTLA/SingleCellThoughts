\documentclass{article}
\usepackage[margin=3cm]{geometry}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{bm}

% document begins here
\begin{document}
\vspace*{0.35in}

% title goes here:
\begin{flushleft}
{\Large
\textbf\newline{Modelling gene-specific technical noise in \textit{scran}}
}
\newline

% authors go here:
%\\
Aaron T. L. Lun\textsuperscript{1,*}
\\
\bigskip
\bf{1} Cancer Research UK Cambridge Institute, University of Cambridge, Li Ka Shing Centre, Robinson Way, Cambridge CB2 0RE, United Kingdom
\\
\bigskip
* aaron.lun@cruk.cam.ac.uk

\end{flushleft}

\section{Overview}
We consider the problem of modelling the technical noise in log-transformed normalized expression values from single-cell RNA-seq (scRNA-seq) data.
We use log-transformed values rather than working on the raw scale, as differences between log-values directly represent log-fold changes.
This is arguably more relevant than differences in the absolute scale of the counts.
The log-transformation also provides some measure of variance stabilization for NB-distributed counts with a constant dispersion but a variable mean \citep{law2014voom}. 
Without it, the trend would be pretty extreme and difficult to fit reliably, especially as the precision of the variance estimate will change with the mean.

% You _could_ downweight the absolute differences based on the standard deviation, in order to adjust for the value of the mean.
% However, this depends on a sensible estimate of the standard deviation - across the population? Subpopulations?
% Also, the log-transformation seems better than square-rooting it, which is the VST for the Poisson only.
% You could play around with various Box-Cox transformations, but the mean-variance trend is pretty extreme and probably won't go away just by changing lambda.

\section{Linear modelling of log-expression values}
Here we describe the basic process of variance estimation.
Let $y_{ig}$ be the log-expression value of gene $g$ in cell $i$.
Denote the vector of log-expression values across all cells for $g$ as $\bm{y}_g$, with length equal to the number of cells $n$.
Assume that this can be expressed as a linear sum of $p$ predictors
\[
\bm{y}_g = \bm{X}\bm{\beta}_g + \bm{\epsilon}_{ig} \;,
\]
where $\bm{X}$ is a $n\times p$ matrix specifying the value of each predictor and each cell;
$\bm{\beta}_g$ is a vector of coefficients, one for each predictor ($p$ in total);
and $\bm{\epsilon}_{g}$ is a vector of random variables $\epsilon_{ig}$, representing the error for each observation.
We assume that 
\[
    \epsilon_{ig} \sim \mbox{Normal}(0, \sigma^2_g)
\]
and is independently sampled for each observation.
We also write
\[
\bm{\mu}_g = \bm{X}\bm{\beta}_g
\]
where $\bm{\mu}_g$ is a vector of length $n$ containing mean log-expression values $\mu_{ig} = E(y_{ig})$.

For each gene, we fit a linear model to $\bm{y}_g$ using standard least-squares methods.
In the fitted model, denote $\hat\mu_{ig}$ as the fitted value for each observation, with
\[
\bm{\hat\mu}_{g} = \bm{X} \bm{\hat\beta}_g
\]
where $\bm{\hat\beta}_g$ contains gene-specific estimates for the coefficients.
We estimate $\sigma^2_g$ with
\[
\hat\sigma^2_g = \frac{\sum_{i=1}^{n} (y_{ig} - \hat\mu_{ig})^2}{n-p} \;.
\] 
In this manner, we obtain a variance estimate for each gene in the data set.
We also obtain an average abundance $A_g$ for each gene by averaging $y_{ig}$ over all $i$.

\section{Modelling the distribution of variance estimates}
Consider a set of ``control'' genes that are \textit{not} highly variable. 
Most commonly, each control gene $s$ is a spike-in transcript that has been added at the same quantity to each cell, though other definitions are possible.
As $\epsilon_{is}$ are normally distributed, we have
\[
    \hat\sigma^2_s|\sigma^2_s \sim \frac{\sigma^2_s\chi^2_{n-p}}{n-p} \;.
\]
Further assume that $\sigma^2_s$ are distributed around a mean-variance trend $V(.)$ as
\[
\sigma^2_s \sim V(A_s) d_0 \chi^{-2}_{d_0}
\]
where $d_0$ is the prior degrees of freedom \citep{smyth2004linear}.
This quantifies the variance in $\sigma^2_s$ due to gene-specific factors such as GC content or length or secondary structure affecting capture efficiency or amplification noise.
From these two expressions, we obtain
\begin{equation}
    \hat\sigma^2_s \sim V(A_s)F(n-p, d_0) \label{eqn:estdist}
\end{equation}
which allows us to estimate $d_0$ and $V(.)$ from the observed distribution of $\hat\sigma^2_s$.

\section{Estimating the distribution parameters}

\subsection{Overview}
To estimate the parameters of the $\sigma^2_s$ distribution, we fit a trend $f(.)$ to $\log(\hat\sigma^2_s)$ against $A_s$.
We use log-transformed values as $\exp[f(.)]$ guarantees positive values for $V(.)$.
The fitted value of the trend for each $s$ is an estimate of the expectation of the log-values for genes with similar $A_s$, denoted as
\[
    f(A_s) \approx E_{|A_s}[\log(\hat\sigma^2_s)] = \log[V(A_s)] + E\{\log[F(n-p, d_0)]\} \;.
\]
Dividing $\hat\sigma^2_s$ by $\exp[f(A_s)]$ yields a scaled F-distribution independent of $A_s$, i.e.,
\[
    \frac{\hat\sigma^2_s}{\exp[f(A_s)]} \sim \Phi F(n-p, d_0) \;,
\]
where $\Phi = \exp(E\{\log[F(n-p, d_0)]\})^{-1}$. 
Both $d_0$ and $\Phi$ can therefore be determined by robustly fitting an F-distribution to the above ratios \citep{phipson2016robust}, using the \texttt{fitFDistRobustly} function.
Multiplication of $\exp[f(.)]$ with $\Phi$ then yields an estimate of $V(.)$.

\subsection{Prefitting with a parametric curve}
In practice, we use a two-step procedure to fit $f(.)$.
The first step uses a parametric curve that models the distinct shape of the mean-variance relationship for log-expression values in scRNA-seq data.
This curve in the variance is defined with respect to the mean $x$ with the function
\begin{equation}
    f_1(x) = \frac{A x}{x^M + B} \label{eqn:paramtrend}
\end{equation}
with $A > 0$, $M > 1$ and $B > 0$.
We chose this function as it captures major features of the mean-variance trend including a linear increase from zero at small positive $x$, a smooth peak at moderate $x$ and an asymptotic decrease to zero as $x \to \infty$.

We use non-linear least-squares optimization to fit $f_1(.)$ to $\hat\sigma^2_s$ against $A_s$.
Sensible starting values for each parameter are obtained as follows:
\begin{enumerate}
\item The approximate location ($\hat{x}_m$) and height ($\hat{y}_m$) of the peak in the curve is obtained by fitting a loess smoother to $\sigma^2_s$.
\item The initial gradient $\hat{g}$ is obtained by fitting a line to the variances for controls where $A_s$ is between $\hat{x}_m/2$ and the minimum $A_s$ across all controls.
The use of this subset of controls avoids the non-linearity as the curve approaches the peak.
\item Near zero, Equation~\ref{eqn:paramtrend} becomes $Ax/B$ when $M>1$ and $B>0$. 
Thus, if $B$ is known, $A$ can be computed by multiplying $B$ with $\hat{g}$.
\item The stationary point at Equation~\ref{eqn:paramtrend} occurs when $x^M = B/(M-1)$.
This allows us to solve for $B$ when $M$ is known, by replacing $x$ with our estimate of the peak location $\hat{x}_m$.
\item In theory, the height of the peak could be used to solve for $M$. 
However, this seems unstable in practice, so instead a grid search is used to identify the $M$ (and subsequently the $B$, and therefore the $A$) that minimizes the sum of squares.
\end{enumerate}
The resulting values of $M$, $A$ and $B$ are used in the \texttt{nls} function to refine the curve fit.
Searching is performed in the log-space to ensure that the constraints on $A$, $M$ and $B$ are respected.

In the second step, we remove most of the effect of the mean-variance curve by computing 
\[
    r_s = \hat\sigma^2_s/f_1(A_s) \;.
\]
We then fit a robust loess curve $f_2(.)$ to $\log(r_s)$ against $A_s$, and define
\[
    f(A_s) = \log[f_1(A_s)] + f_2(A_s)
\]
The second fit accounts for empirical variations from the relationship in Equation~\ref{eqn:paramtrend} and reduces the effect of any outliers.
It also ensures that the fit is ultimately determined by the sum of squares in the log-space, consistent with the original definition of $f(.)$ fitted to the log-variances.
The use of a two-step protocol is necessary because a complex trend with strong gradients is difficult to fit directly with smoothing algorithms.
By first using a parametric curve to model the complex parts of the trend, we can factor out the strong gradients to improve the accuracy of the loess smoothing.

\section{Modelling the technical noise for endogenous genes}

\subsection{Decomposing the variance components}
Consider an endogenous gene $g$, which has a true log-expression of $y_{ig}^*$ in cell $i$.
Let $\mbox{var}(y_{ig}^*) = \sigma^2_{(b)g}$, representing the underlying biological variation.
The observed log-expression $y_{ig}$ is then sampled from $y_{ig}|y_{ig}^* \sim T_{ig}$ with $E(T_{ig}) = y^*_{ig}$, representing the effect of technical noise.
The observed variance is 
\begin{align*}
    \sigma^2_g &= \mbox{var}[E(y_{ig}|y_{ig}^*)] + E[\mbox{var}(y_{ig}|y_{ig}^*)] \\
               &= \sigma^2_{(b)g} +  E[\mbox{var}(y_{ig}|y_{ig}^*)]  \;.
\end{align*}
Denote $E[\mbox{var}(y_{ig}|y_{ig}^*)]$ as $\sigma^2_{(t)g}$.
For a gene with abundance $A_g$, we estimate $\sigma^2_{(t)g}$ from the corresponding control genes.
Specifically, we use the expectation of $\sigma^2_s$ at this abundance (Equation~\ref{eqn:estdist}), i.e., 
\begin{equation}
    \hat\sigma^2_{(t)g} \approx E_{|A_g}[\sigma^2_s] = \frac{d_0V(A_g)}{d_0-2} \;. \label{eqn:techest}
\end{equation}
We can simply obtain an estimate of the biological component as
\[
    \hat\sigma^2_{(b)g} = \hat\sigma^2_g - \hat\sigma^2_{(t)g} \;,
\]
allowing the observed variance to be decomposed to its biological and technical components.

\subsection{Detecting highly variable genes}
Our null hypothesis for endogenous gene $g$ is that its variance estimate is distributed according to Equation~\ref{eqn:estdist}.
We test this null hypothesis with a one-sided $F$-test.
Specifically, we compute
\[
    F_g = \frac{\hat\sigma^2_g}{V(A_g)} \;.
\]
The $p$-value for this gene is the upper tail probability at $F_g$ for an $F$-distribution with $n-p$ and $d_0$ degrees of freedom.
Highly variable genes are then defined at a false discovery rate threshold.

\subsection{Comparing variances of controls and endogenous genes}
Using the variance of the control genes assumes that, at the very least, $\mbox{var}(y_{ig}|y_{ig}^*) = \mbox{var}(y_{is})$ when $E(y_{is}) = y_{ig}^*$, 
i.e., the sampling noise is the same for control and endogenous genes at the same true log-expression.
The strength of this assumption depends on the properties of the control genes.

The quality of the approximation in Equation~\ref{eqn:techest} depends on how strongly $\mbox{var}(y_{ig}|y_{ig}^*)$ changes as a function of $y^*_{ig}$.
If it does not change, the expression is exact; however, this unlikely to be the case as the sampling noise for counts will depend on the mean.
The approximation is also less likely to be accurate if there are strong changes with respect to $y_{ig}^*$ \textit{and} the variance of $y_{ig}^*$ across cells is high.
Nonetheless, this approach still provides a useful approximation for modelling technical noise.

\subsection{Comparing abundances of controls and endogenous genes}
Ensuring comparability of the average abundances of the control and endogenous genes is not straightforward.
Different size factors are generally used for control and endogenous genes prior to calculation of log-normalized expression values.
Size factors only have a relative interpretation within each set, and there is no general rationale for adjusting size factors between sets.
To resolve this, we assume that the magnitude of the underlying counts drives the mean-variance relationship.
Thus, control or endogenous genes with the same average count should have the same average abundances.
We center both sets of size factors so that $E(y_{is}/f'_i) \approx E(y_{ig}/f_i)$ if $E(y_{is})=E(y_{ig})$ for spike-in size factor $f'_i$ and endogenous size factor $f_i$ (assuming size factors are known and accurate representations of the scaling bias).
This yields similar values upon taking the mean of the log-normalized expression.

Of course, the mean of the log-values -- i.e., the average abundance -- is not the same as the logarithm of the mean.
This means that the average abundance may not be the same between control and endogenous genes, even if their mean normalized counts were equal.
In practice, the Taylor series approximation indicates that the size of the discrepancy depends on the difference in the variances of the normalized counts between the control and endogenous genes.
If the variances are very different, it is debatable whether it makes any sense to use the average count as a measure of location for the counts in the first place.
Indeed, the overall average may not be meaningful at all in the presence of hidden structured variation.
Average abundances specific to each cell type would be more appropriate, though this is difficult as the cell types are not known at this point.
Moreover, the variance of the endogenous gene should be larger than the control gene at the same mean normalized count.
Even if there were an increase in variance large enough to cause a discrepancy in the average abundances,
the resulting mis-estimation of technical noise should be negligible relative to the total variance.

% collected.m <- collected.v <- numeric(100)
% for (x in seq_along(collected.m)) {
%     y <- log2(rnbinom(100000, mu=10, size=x/10)+1)
%     collected.m[x] <- mean(y)
%     collected.v[x] <- var(y)
% }
% plot(collected.m, collected.v)

\section{Handling uninteresting factors of variation}

\subsection{Blocking in a linear model}
The most obvious approach is to compute the residual variance from a linear model that blocks on unwanted factors of variation.
However, this is not sufficient as it fails to consider the effect of those factors on the average abundance.
Consider a situation with a batch effect that increases the expression of gene $g$ in the second batch.
The technical component for $g$ should be a (weighted) average of the fitted value of the trend at the batch-specific average abundances.
However, we take the fitted value at the the overall average instead, which is not guaranteed to be equivalent or even similar.

\subsection{Computing batch-specific statistics}
A better strategy is to compute variances and average abundances separately for each batch.
This is first performed for the control genes, yielding $\sigma^2_{s,b}$ and $A_{s,b}$ for the sample variance and mean of control $s$ in batch $b$.
Estimates for all $(s, b)$ combinations are pooled together and a single trend is fitted to the variances against the mean.
Next, we repeat this process for all genes, yielding $\sigma^2_{g,b}$ and $A_{g,b}$ for each gene and batch.
The fitted value of the trend at $A_{g,b}$ is used as the batch-specific technical component, which is subtracted from $\sigma^2_{g,b}$ to obtain a batch-specific biological component.
Weighted averages are then calculated to obtain a single estimate per gene, where weighting is performed based on the number of cells (for abundance) or residual d.f. (for variance components) per batch.

The obvious benefit of this strategy is that it accommodates differences in the batch-specific technical noise.
It also protects against \textit{random} shifts in the average abundance of each control gene due to a batch effect.
In contrast, had we used a linear model, we would effectively be fitting to data where both abundance and variance are averaged across batches.
This would only be correct if the trend were linear, which it is not;
or if the batch effect is small, in which case we would not need to bother modelling it.
We avoid such problems by fitting the trend to the batch-specific values.

It should be noted that $p$-values are also computed for each batch separately and need to be combined for each gene.
We use Fisher's method to do so, under the assumption that each batch is independent of the others.
This will test the global null hypothesis that the gene is not highly variable in any batch.
Another option is to use Stouffer's Z method, which tests the same null hypothesis but also allows weighting of each batch by the residual d.f. used for variance estimation.
Stouffer's method is less sensitive than Fisher's method to low $p$-values in a few batches, favouring rejection when $p$-values are low for many batches.
However, we prefer Fisher's method precisely for this reason, as each batch may only capture a subset of the biological heterogeneity (e.g., cell types).
Significant heterogeneity in any batch should be sufficient to reject the global null for the genes involved.

The obvious disadvantage of this approach is that it only works with a single blocking factor.
Additive models or covariates require the simpler linear modelling scheme described above.

\subsection{Fitting batch-specific trends}
The previous strategy uses batch-specific statistics with a single fitted trend.
This is not appropriate if the trend itself differs between batches.
In such cases, the only option is to split the dataset into its constituent batches, model the variance in each batch separately, and combine the results across batches.
This uses a batch-specific trend fit, which avoids distorted estimates of the technical noise in each batch.
Combining of batch-specific statistics for each gene is done using the same approach described above, i.e., weighted averages for variances and abundances, Fisher's method for $p$-values.

One of the most common causes of differences in the trend is scaling normalization when the coverage of control or endogenous genes differs systematically between batches.
Consider a dataset containing two batches, where the second batch was sequenced to twice the depth of the first.
Further assume that there are no other biases in this dataset and that sequencing is the only source of variability. 
Using batch-specific statistics on the log-counts (without any normalization), the same mean-variance trend would manifest in both batches,
where the data points for the second batch are shifted to the right along the trend compared to the data points for the first batch.
However, scaling normalization will reduce the average abundance of the data points in the second batch,
shifting them back to the left \textit{but not necessarily along the trend}.
This effectively results in a different trend in each batch.

(Incidentally, this is why the use of batch-specific statistics alone only protects against random shifts in the batch-specific average abundances.
Random shifts in both directions should not change the distribution of size factors in each batch, avoiding problems with scaling normalization.)

Another important consideration is that the calculation of normalized values need to be repeated in each batch.
Size factors are computed globally, and an arbitrary subset is not guaranteed to be centered at unity.
As such, they need to be re-centered within each batch prior to calculation of log-normalized values and modelling of the variance.
See above for further details on size factor centering.

\section{Further comments}

\subsection{Comparison to the Brennecke method}
Our method uses the normalized log-expression values as $y_{ig}$, whereas the Brennecke method operates on the CV$^2$ values calculated from the (normalized) counts.
Both methods are similar with respect to correctly identifying genes above the trend (see \texttt{simulations/power/above\_*}).
Their associated hypothesis tests are also similar in terms of the number of HVGs detected in a variety of simulation scenarios (see \texttt{simulations/power/detect\_*}).
The main differences are:
\begin{itemize}
\item Our method returns fewer false positives, possibly because log-expression values are somewhat more normal-looking. 
Better modelling the variability of the true variances around the mean-variance trend is probably not a factor as all $d_0$ estimates seem to be infinite.
\item Our method is better than the Brennecke method for detecting rare populations characterized by downregulation of genes.
This is attributable to the fact that decreasing the mean increases the technical CV$^2$, making it harder to observe an increase in the biological CV$^2$.
\item The Brennecke method is better at detecting HVGs that drive weak differences between small clusters, independent of its anticonservativeness (look at the top genes).
This is probably because a fold-increase has less effect on the variance after a log-transformation.
\end{itemize}

From a practical perspective, we prefer computing the variance of log-values as this improves consistency with downstream procedures.
Dimensionality reduction, clustering and visualization are all applied on the log-expression values.
Selecting HVGs on a different metric may lead to inconsistencies if they are not also highly variable in the log-space.
For example, an outlier-driven HVG selected with a large CV$^2$ looks underwhelming when visualized with log-expression values.
Another example is provided in \texttt{simulations/power/sim\_comp.R}, where detection power differs between methods at high and low abundances.
The use of Brennecke's method would favour genes that have low biological components in the log-space, reducing discriminatory power in the presence of technical noise.

\subsection{Filtering out low-abundance genes}
At very low abundances, counts are limited to values of 0 or 1.
This means that the log-expression values effectively follow a scaled binomial distribution, for which the relationship between the sample mean and variance estimate is exact.
In other words, all low-abundance genes lie exactly on the empirical mean-variance trend with no scatter.
When fitting $f(.)$, this understates the variance around the trend and leads to overestimation of $d_0$ for all other genes.
The preponderance of low-abundance genes also interferes with span-based loess smoothing at higher abundances.
Thus, filtering of these genes should be performed prior to trend fitting.
The simulations in \texttt{simulations/filter/} indicate that a threshold of 0.1 on the $\log_2(y_{ig}+1)$ mean should be used.
Below this threshold, discreteness in the variance estimates begins to dominate.

\subsection{Avoiding distributional assumptions}
Currently, the above framework assumes normality for the log-expression values, which is probably unreasonable.
The trend fitting is unlikely to be strongly affected as non-normality should not change the expectation of sample variances.
However, the $p$-value from the F-test is likely to be invalidated by strong non-normality, as the distribution of the sample variances will change.

A better approach would be to take advantage of the central limit theorem for the variance estimates themselves.
This would allow us to assume that $\hat\sigma^2_s|\sigma^2_s \sim \sigma^2_s N(1, \psi_0)$ where $\psi_0$ represents the sampling variance of the variance.
The use of the CLT should be possible as the variance estimates are simply the sum of squared residuals.
(The lost degrees of freedom can effectively be ignored for large $n$ and small $p$.)
We can model $\sigma^2_s \sim V'(A_s)N(1, \psi_1)$ for a mean-variance trend $V'(.)$, where $\psi_1$ represents the spread of true variances (equivalent to $d_0$).
This would yield $\hat\sigma^2_s \sim V'(A_s)N(1, \psi_1 + \psi_0)$.

$V'(.)$ can be defined as $\exp[f(.)]$, as described above.
In theory, this is not entirely correct as the scaling of $f(.)$ makes different distributional assumptions about $\hat\sigma^2_s$.
Nonetheless, the differences are likely to be small, and in practice, fitting a trend directly to $\hat\sigma^2_s$ is risky due to the potential for negative fitted values.
A log-transformation is not possible with this approach as the expectation of a log-transformed normal variate does not exist (and thus cannot be used to adjust the scale).

The more relevant point is to see whether hypothesis testing is improved.
The null hypothesis for gene $g$ states that $\hat\sigma^2_g  \sim V'(A_g)N(1, \psi_1 + \psi_0)$, assuming that control and endogenous genes are comparable.
While a $z$-test is the most obvious strategy here, it is also possible to use a one-sided $t$-test to account for the uncertainty of estimating $\psi_1 + \psi_0$.
This seems to yield satisfactory type I error control in a variety of situations (see the results in \texttt{simulations/alpha/}).


\bibliography{ref}
\bibliographystyle{plainnat}

\end{document}
