\documentclass{article}
\usepackage[margin=3cm]{geometry}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{bm}

% document begins here
\begin{document}
\vspace*{0.35in}

% title goes here:
\begin{flushleft}
{\Large
\textbf\newline{Choosing the number of principal components}
}
\newline

% authors go here:
%\\
Aaron T. L. Lun\textsuperscript{1,*}
\\
\bigskip
\bf{1} Cancer Research UK Cambridge Institute, University of Cambridge, Li Ka Shing Centre, Robinson Way, Cambridge CB2 0RE, United Kingdom
\\
\bigskip
* aaron.lun@cruk.cam.ac.uk

\end{flushleft}

\section{Overview}
Principal components analysis (PCA) is usually performed to obtain a small set of principal components (PCs) for use in clustering and dimensionality reduction.
The aim is to reduce the random technical noise while preserving the biological signal in the first few PCs.
Decreasing the number of dimensions also reduces computational work, which is especially important for some algorithms, e.g., $k$-nearest neighbour searching.
The challenge here is how to choose an appropriate number of PCs.

\section{Based on technical noise}

\subsection{Description of method}

We assume that the biological signal is wholly captured by the first few PCs.
This is based on the fact that biological processes will generate non-zero covariance between multiple genes that are involved in those processes.
If the loading vector is parallel to one of the basis vectors of the biological subspace, it will explain a component of the variance for every related gene.
When summed across all genes, this will explain a large proportion of the total variance in the data set.

In contrast, technical noise should be mostly present in the later PCs.
This is because technical noise is expected to be random and uncorrelated across genes.
Any one loading vector can only explain the variance for a single gene (or a few, in cases with many genes where non-zero correlations occur by chance).
Thus, each vector should only explain a small proportion of the variance.

We describe elsewhere (see \verb!../hvg_detection/!) how to obtain an estimate of the technical component of the variance for each gene.
This yields an estimate of the total technical variance $\sigma^2_t$ in the data set.
We choose the number of PCs as the smallest value $k$ such that 
\[
    \sum_{l=k+1}^N s^2_l  \le \sigma^2_t \;,
\]
where $s^2_l$ describes the variance explained by the $l$\textsuperscript{th} PC and $N$ is the total number of PCs.
This serves as a lower bound on the number of PCs required to preserve biology while removing technical noise.

% Additivity of gene-wise variances in PCA just reflects the fact that we're looking at high-dimensional variance.
% This has nothing to do with the independence of the genes (which is only relevant when you want to compute the variance of the sum).
% Or in other words; we're just computing the squared distances of each cell from the center (origin).
% This gives the same result if you do it all at once or sum the per-gene results.

In practice, we only use genes where the total variance is greater than the estimated technical component.
This ensures that there is some value of $k \in [1, N]$ that will satisfy the above inequality.
It also reduces random noise that can skew the loading vectors for the first PCs if the biological signal is weak.
However, it likely will result in some overestimation of the biological noise and of $k$.

\subsection{Justifying the lower bound} 
The choice of $k$ is a lower bound as it assumes that the first $k$ PCs contain \textit{only} biological variability.
However, even for the earliest PCs, the loading vector will capture some aspect of the technical noise that happens to be parallel to the biological subspace.
This means that the variance explained by these PCs will contain a non-zero technical component.
Conversely, some of the biological variability must be explained by later PCs, as it cannot be fully accounted for in the first $k$ PCs.

When the technical noise is high, the loading vector can be skewed to capture the largest components of the noise.
This means that the earlier PCs tend to have the largest technical components, even though they also capture the biological signal.
Such an effect will increase the discrepancy between our choice of $k$ and the ``true'' number of PCs that needs to be retained to preserve the biological variability.
Of course, it is debatable whether obtaining the true number of PCs is desirable for noisy data, as this will reduce the effectiveness of denoising and dimensionality reduction. 

\subsection{An alternative for a tighter bound}
If we weaken the assumption that the technical component is zero in the first $k$ PCs, we might obtain a more suitable choice for the number of PCs.
Assume that the first $k$ PCs contain the entirety of the biological signal, but also contribute at least
\[
    k s^2_{k+1} 
\]
to the total technical variance in the data. 
Here, we assume that the technical component explained by a PC is at least as large as that of any later PC (such as the $k+1$\textsuperscript{th} PC).
This is generally reasonable in noisy data where the loading vectors for early PCs are skewed by random noise.
Combined with the noise in the later PCs, the total technical variance must be at least
\[
    k s^2_{k+1} + \sum_{l=k+1}^N s^2_l \;.
\]
One could then choose the smallest $k$ that satisfies
\[
    k^* s^2_{k^*+1} + \sum_{l=k+1}^K s^2_l \le \sigma^2_t  
\]
to obtain a more accurate lower bound on $k$ that preserves biological signal.

In practice, this alternative approach tends to yield very large $k$, which defeats the purpose of denoising and dimensionality reduction.
Moreover, it can be unstable with small changes in $\sigma^2_t$ resulting in large changes to the chosen $k$.
This is because the gradient with respect to $k$ of the LHS of the above inequality can be very small.
For the most extreme case where all $s^2_l$ are very similar, we could obtain near-identical LHS values for a range of $k$.
By comparison, the original approach has a LHS that is supralinear with respect to $k$, such that a well-defined choice of $k$ can be obtained for any $\sigma^2_t$.

\subsection{Use with multi-batch experiments}
This method can still be applied on batch-corrected data provided that an overall estimate of the technical component is available.
Such an estimate can be obtained using the \texttt{combineVar} function after modelling the mean-variance trend within each batch, 
e.g., to account for differences in spike-in abundance across batches.
Another requirement is that the batch correction does not change the residuals within each batch.
Any such distortion of the residuals this would alter the variance estimate, and thus the contribution of technical noise in the corrected data.
Acceptable batch correction methods include simple linear regression (via \texttt{removeBatchEffect}) without any empirical Bayes shrinkage of the variances;
or \texttt{mnnCorrect} with a large \texttt{sigma} to enforce the use of global correction vectors.

\section{With parallel analysis}
Horn's parallel analysis involves permuting each row of the input gene expression matrix and performing PCA on this permuted matrix.
This yields a variance explained for each PC, denoted as $\omega^2_l$ for this permuted matrix.
Any PC with a $\sigma^2_l$ (from the original input matrix) comparable to $\omega^2_l$ is considered to be uninteresting, 
as the PC explains no more variance than expected under a random model containing no structure. 
One can visualize this strategy by considering a scree plot and discarding all PCs past the first intersection of the ``variance explained'' lines for the original and permuted PCAs.

Several definitions of ``comparable'' can be used to define the first uninteresting PC.
The simplest is to remove all PCs past and including the first PC where $\sigma^2_l < \omega^2_l$.
However, this tends to retain too many PCs in noisy datasets where the original and permuted variance-explained curves are similar and intersect slowly.
Another solution is to repeat the permutations many times, and define the threshold as an upper quantile of $\omega^2_l$ for each $l$.
A PC with $\sigma^2_l$ below this quantile is considered to be feasibly sampled from the distribution of $\omega^2_l$ under a random model, and is subsequently discarded.
We use the 90\textsuperscript{th} percentile by default, which yields a more conservative estimate of the number of PCs.

It is tempting to rephrase the quantile-based methodology in terms of $p$-values. 
Specifically, we could compute a ``$p$-value'' for each PC as the proportion of iterations where $\omega^2_l \ge \sigma^2_l$.
However, this is not correct.
Permutations generate a null model of global randomness, which means that the proportion of iterations is \textit{only} interpretable as a $p$-value for the first PC.
The tests for later PCs do not involve an appropriate null hypothesis when the null is already rejected for the first PC.
Indeed, in the presence of structure, $\omega^2_l$ is always systematically larger than $\sigma^2_l$ at later PCs.
This guarantees intersection of the two variance-explained curves but results in $p$-values for the later PCs that are heavily skewed towards 1, 
even if the later PCs do not capture any structure (and thus should correspond to true nulls).
Obtaining a suitable test for PC $l$ would involve permuting the matrix in a manner that preserves the basis vectors that contribute to PCs $\{1, \ldots, l-1\}$; this is not easy.

Additional practical factors complicate the use of the proportions as $p$-values.
One is the need for multiple testing correction across PCs.
This is not straightforward as later PCs are never tested if the null is accepted for an earlier PC.
The exact number of tests is not always the same, nor are they independent. 
Another complication is that normalized counts from libraries of different sizes are not exchangeable via permutation.
The mean-variance relationship is quite different for normalized counts derived from small or large libraries; 
this will affect the accuracy of any permutation test, even for the first eigenvalue where the proportion is ostensibly interpretable as a $p$-value.

\section{Simulation results}
We assessed the performance of our two methods using simulated data with known clusters (see \texttt{simulations/}).
For each pair of clusters, we quantified their separation in the PC space of the chosen dimensions,
by computing the proportion of variance explained (PVE) by the cluster identity (only using cells in that pair of clusters).
We compared methods in terms of their ability to separate all pairs of clusters, using the smallest PVE;
and their average cluster separation, using the median PVE.

The two methods are generally similar in behaviour, though parallel analysis retains more PCs than the technical noise-based method in noisy data sets.
Parallel analysis yields larger minimum PVEs due to the retention of later PCs that capture weak differences between poorly-separated clusters.
However, the average cluster separation is often poorer, probably due to the retention of more random noise with later PCs.
Both methods are better than using all PCs in most simulation scenarios.
Whether average or minimum separation should be maximized depends on the intended resolution of the study.

We note that, in practice, the technical noise-based method is much more convenient, requiring only an estimate of the technical noise for each gene.
It is time-consuming to repeat PCAs for sufficient permutations to characterize the distribution of $\omega^2_l$, especially for large matrices.

\end{document}
