\documentclass{article}
\usepackage[margin=3cm]{geometry}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{bm}

% document begins here
\begin{document}
\vspace*{0.35in}

% title goes here:
\begin{flushleft}
{\Large
\textbf\newline{Choosing the number of principal components}
}
\newline

% authors go here:
%\\
Aaron T. L. Lun\textsuperscript{1,*}
\\
\bigskip
\bf{1} Cancer Research UK Cambridge Institute, University of Cambridge, Li Ka Shing Centre, Robinson Way, Cambridge CB2 0RE, United Kingdom
\\
\bigskip
* aaron.lun@cruk.cam.ac.uk

\end{flushleft}

\section{Overview}
Principal components analysis (PCA) is usually performed to obtain a small set of principal components (PCs) for use in clustering and dimensionality reduction.
The aim is to reduce the random technical noise while preserving the biological signal in the first few PCs.
Decreasing the number of dimensions also reduces computational work, which is especially important for some algorithms, e.g., $k$-nearest neighbour searching.
The challenge here is how to choose an appropriate number of PCs.

\section{Based on technical noise}
\label{sec:noise}

\subsection{Description of method}

We assume that the biological signal is wholly captured by the first few PCs.
This is based on the fact that biological processes will generate non-zero covariance between multiple genes that are involved in those processes.
If the loading vector is parallel to one of the basis vectors of the biological subspace, it will explain a component of the variance for every related gene.
When summed across all genes, this will explain a large proportion of the total variance in the data set.

In contrast, technical noise should be mostly present in the later PCs.
This is because technical noise is expected to be random and uncorrelated across genes.
Any one loading vector can only explain the variance for a single gene (or a few, in cases with many genes where non-zero correlations occur by chance).
Thus, each vector should only explain a small proportion of the variance.

We describe elsewhere (see \verb!../hvg_detection/!) how to obtain an estimate of the technical component of the variance for each gene.
This yields an estimate of the total technical variance $\sigma^2_t$ in the data set.
We choose the number of PCs as the smallest value $k$ such that 
\[
    \sum_{l=k+1}^N s^2_l  \le \sigma^2_t \;,
\]
where $s^2_l$ describes the variance explained by the $l$\textsuperscript{th} PC and $N$ is the total number of PCs.
This serves as a lower bound on the number of PCs required to preserve biology while removing technical noise.

% Additivity of gene-wise variances in PCA just reflects the fact that we're looking at high-dimensional variance.
% This has nothing to do with the independence of the genes (which is only relevant when you want to compute the variance of the sum).
% Or in other words; we're just computing the squared distances of each cell from the center (origin).
% This gives the same result if you do it all at once or sum the per-gene results.

In practice, we only use genes where the total variance is greater than the estimated technical component.
This ensures that there is some value of $k \in [1, N]$ that will satisfy the above inequality.
It also reduces random noise that can skew the loading vectors for the first PCs if the biological signal is weak.
However, it likely will result in some overestimation of the biological noise and of $k$.

\subsection{Justifying the lower bound} 
The choice of $k$ is a lower bound as it assumes that the first $k$ PCs contain \textit{only} biological variability.
However, even for the earliest PCs, the loading vector will capture some aspect of the technical noise that happens to be parallel to the biological subspace.
This means that the variance explained by these PCs will contain a non-zero technical component.
Conversely, some of the biological variability must be explained by later PCs, as it cannot be fully accounted for in the first $k$ PCs.

When the technical noise is high, the loading vector can be skewed to capture the largest components of the noise.
This means that the earlier PCs tend to have the largest technical components, even though they also capture the biological signal.
Such an effect will increase the discrepancy between our choice of $k$ and the ``true'' number of PCs that needs to be retained to preserve the biological variability.
Of course, it is debatable whether obtaining the true number of PCs is desirable for noisy data, as this will reduce the effectiveness of denoising and dimensionality reduction. 

\subsection{An alternative for a tighter bound}
If we weaken the assumption that the technical component is zero in the first $k$ PCs, we might obtain a more suitable choice for the number of PCs.
Assume that the first $k$ PCs contain the entirety of the biological signal, but also contribute at least
\[
    k s^2_{k+1} 
\]
to the total technical variance in the data. 
Here, we assume that the technical component explained by a PC is at least as large as that of any later PC (such as the $k+1$\textsuperscript{th} PC).
This is generally reasonable in noisy data where the loading vectors for early PCs are skewed by random noise.
Combined with the noise in the later PCs, the total technical variance must be at least
\[
    k s^2_{k+1} + \sum_{l=k+1}^N s^2_l \;.
\]
One could then choose the smallest $k$ that satisfies
\[
    k^* s^2_{k^*+1} + \sum_{l=k+1}^K s^2_l \le \sigma^2_t  
\]
to obtain a more accurate lower bound on $k$ that preserves biological signal.

In practice, this alternative approach tends to yield very large $k$, which defeats the purpose of denoising and dimensionality reduction.
Moreover, it can be unstable with small changes in $\sigma^2_t$ resulting in large changes to the chosen $k$.
This is because the gradient with respect to $k$ of the LHS of the above inequality can be very small.
For the most extreme case where all $s^2_l$ are very similar, we could obtain near-identical LHS values for a range of $k$.
By comparison, the original approach has a LHS that is supralinear with respect to $k$, such that a well-defined choice of $k$ can be obtained for any $\sigma^2_t$.

\subsection{Use with multi-batch experiments}
This method can still be applied on batch-corrected data provided that an overall estimate of the technical component is available.
Such an estimate can be obtained using the \texttt{combineVar} function after modelling the mean-variance trend within each batch, 
e.g., to account for differences in spike-in abundance across batches.
Another requirement is that the batch correction does not change the residuals within each batch.
Any such distortion of the residuals this would alter the variance estimate, and thus the contribution of technical noise in the corrected data.
Acceptable batch correction methods include simple linear regression (via \texttt{removeBatchEffect}) without any empirical Bayes shrinkage of the variances;
or \texttt{mnnCorrect} with a large \texttt{sigma} to enforce the use of global correction vectors.

\section{With parallel analysis}
Horn's parallel analysis involves permuting each row of the input gene expression matrix and performing PCA on this permuted matrix.
This yields a variance explained for each PC, denoted as $\omega^2_l$ for this permuted matrix.
Any PC with a $\sigma^2_l$ (from the original input matrix) comparable to $\omega^2_l$ is considered to be uninteresting, 
as the PC explains no more variance than expected under a random model containing no structure. 
One can visualize this strategy by considering a scree plot and discarding all PCs past the first intersection of the ``variance explained'' lines for the original and permuted PCAs.

Several definitions of ``comparable'' can be used to define the first uninteresting PC.
The simplest is to remove all PCs past and including the first PC where $\sigma^2_l < \omega^2_l$.
However, this tends to retain too many PCs in noisy datasets where the original and permuted variance-explained curves are similar and intersect slowly.
Another solution is to repeat the permutations many times, and define the threshold as an upper quantile of $\omega^2_l$ for each $l$.
A PC with $\sigma^2_l$ below this quantile is considered to be feasibly sampled from the distribution of $\omega^2_l$ under a random model, and is subsequently discarded.
We use the 90\textsuperscript{th} percentile by default, which yields a more conservative estimate of the number of PCs.

It is tempting to rephrase the quantile-based methodology in terms of $p$-values. 
Specifically, we could compute a ``$p$-value'' for each PC as the proportion of iterations where $\omega^2_l \ge \sigma^2_l$.
However, this is not correct.
Permutations generate a null model of global randomness, which means that the proportion of iterations is \textit{only} interpretable as a $p$-value for the first PC.
The tests for later PCs do not involve an appropriate null hypothesis when the null is already rejected for the first PC.
Indeed, in the presence of structure, $\omega^2_l$ is always systematically larger than $\sigma^2_l$ at later PCs.
This guarantees intersection of the two variance-explained curves but results in $p$-values for the later PCs that are heavily skewed towards 1, 
even if the later PCs do not capture any structure (and thus should correspond to true nulls).
Obtaining a suitable test for PC $l$ would involve permuting the matrix in a manner that preserves the basis vectors that contribute to PCs $\{1, \ldots, l-1\}$; this is not easy.

Additional practical factors complicate the use of the proportions as $p$-values.
One is the need for multiple testing correction across PCs.
This is not straightforward as later PCs are never tested if the null is accepted for an earlier PC.
The exact number of tests is not always the same, nor are they independent. 
Another complication is that normalized counts from libraries of different sizes are not exchangeable via permutation.
The mean-variance relationship is quite different for normalized counts derived from small or large libraries; 
this will affect the accuracy of any permutation test, even for the first eigenvalue where the proportion is ostensibly interpretable as a $p$-value.

\section{Using the Marchenko-Pastur limit}
The Marchenko-Pastur law describes the asymptotic distribution of singular values for a large rectangular matrix containing i.i.d.\ random variables.
This distribution has a strict upper bound that could theoretically be used as a threshold on the number of PCs to retain.
To justify this procedure, we assume that our rectangular expression matrix can be written as $Y = T + X$, 
where $T$ is a $r$-rank matrix of true signal and $X$ is a matrix of i.i.d.\ noise.
We further assume that the first $r$ eigenvectors of the covariance matrix are exactly linear combinations of the $r$ basis vectors of $T$,
i.e., the additional noise due to $X$ does not affect the identification of the true basis vectors.
This means that, once the first $r$ eigenvectors are regressed out of $Y$, the residual matrix is equal to $R$.
Thus, we can determine $r$ by taking all PCs with singular values greater than the Marchenko-Pastur limit for $R$.

Needless to say, both of the above assumptions are dubious.
The first $r$ basis vectors will rarely be well-represented by the first $r$ eigenvectors in noisy datasets.
Instead, some of the variance in the first $r$ basis vectors will be captured by the $>r$ eigenvectors.
This generally results in an overstatement of the number of PCs to retain as the later singular values will be increased above the Marchenko-Pastur limit.
Similarly, the assumption of i.i.d.\ noise does not hold in real scRNA-seq data where the technical component varies with the mean.
If there are a few features with high variability, the first singular value of $R$ will be greater than expected by the Marchenko-Pastur law.
This again results in overstatement of the number of retained PCs if the Marchenko-Pastur limit is used. 

The practical use of this method would require a mean estimate of the technical noise across all genes.
The square root of this value would then be used to scale the Marchenko-Pastur limit to reflect the scaling of $R$.
In theory, we could also scale the expression for each individual feature by the reciprocal of the technical component of that feature.
This would ensure that the technical noise is roughly i.i.d.\ across features prior to PCA.
However, scaling will distort the underlying biological differences in the PCA results (see \verb!../../general/scaling.Rmd!).
For example, small biological components will be strongly amplified if the corresponding genes also have small technical components.
This is not desirable as such genes were not originally driving heterogeneity in the data.

\section{Using the Gavish-Donoho method}
Gavish and Donoho (2013) describe a method for determining the optimal threshold for the singular values, above which all components should be retained.
Optimality is defined in terms of accuracy of the low-rank reconstruction from the retained components -- see Section~\ref{sec:simulations} for more details.
Their method assumes i.i.d.\ noise with a constant standard deviation, so many of the considerations described above for the Marchenko-Pastur method also apply here.
In particular, we can use the mean technical component to provide a practical estimate of the constant noise for calculation of the threshold.

\section{Simulation results}
\label{sec:simulations}
We assessed the performance of these methods using a variety of simulations (see \verb!simulations!).
Recall that our aim is to achieve dimensionality reduction and denoising, not to interpret the individual components or factors.
Thus, we are only concerned with obtaining an accurate estimate of the true signal matrix, using low-rank approximations of varying rank from the PCA.
Accuracy is assessed by computing the mean squared error (MSE) of the low-rank reconstruction from the known true signal.
The MSE is useful as an overall measure of accuracy as it combines the error due to bias, which decreases with more components; and loss of precision, which increases with more components.

The Gavish-Donoho method is the best performer (i.e., minimizes the MSE) when the technical noise is constant or moderately variable across genes, as theoretically expected.
When noise is highly variable across genes, the Gavish-Donoho method seems to strongly underestimate the number of components.
Parallel analysis and the Marchenko-Pastur method routinely overestimate the number of components to retain.
By comparison, our noise-based method in Section~\ref{sec:noise} typically underestimates the number of components - this is again expected, as we are computing a lower bound.
While it is often suboptimal, our method does provide reasonably stable performance in multiple conditions.

We note that, in practice, the technical noise-based method is much more convenient, requiring only an estimate of the technical noise for each gene.
It is time-consuming to repeat PCAs for sufficient permutations to characterize the distribution of $\omega^2_l$, especially for large matrices.

\end{document}
