\documentclass{article}
\usepackage[margin=3cm]{geometry}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{bm}

% document begins here
\begin{document}
\vspace*{0.35in}

% title goes here:
\begin{flushleft}
{\Large
\textbf\newline{Modelling gene-specific technical noise in \textit{scran}}
}
\newline

% authors go here:
%\\
Aaron T. L. Lun\textsuperscript{1,*}
\\
\bigskip
\bf{1} Cancer Research UK Cambridge Institute, University of Cambridge, Li Ka Shing Centre, Robinson Way, Cambridge CB2 0RE, United Kingdom
\\
\bigskip
* aaron.lun@cruk.cam.ac.uk

\end{flushleft}

\section{Overview}
We consider the problem of modelling the technical noise in log-transformed normalized expression values from single-cell RNA-seq (scRNA-seq) data.
We use log-transformed values rather than working on the raw scale, as differences between log-values directly represent log-fold changes.
This is arguably more relevant than differences in the absolute scale of the counts.
The log-transformation also provides some measure of variance stabilization for NB-distributed counts with a constant dispersion but a variable mean \citep{law2014voom}. 
Without it, the trend would be pretty extreme and difficult to fit reliably, especially as the precision of the variance estimate will change with the mean.

% You _could_ downweight the absolute differences based on the standard deviation, in order to adjust for the value of the mean.
% However, this depends on a sensible estimate of the standard deviation - across the population? Subpopulations?
% Also, the log-transformation seems better than square-rooting it, which is the VST for the Poisson only.
% You could play around with various Box-Cox transformations, but the mean-variance trend is pretty extreme and probably won't go away just by changing lambda.

\section{Linear modelling of expression values}
Here we describe the basic process of variance estimation.
Let $y_{ig}$ be the (log-)expression value of gene $g$ in cell $i$.
Denote the vector of expression values across all cells for $g$ as $\bm{y}_g$, with length equal to the number of cells $n$.
Assume that this can be expressed as a linear sum of $p$ predictors
\[
\bm{y}_g = \bm{X}\bm{\beta}_g + \bm{\epsilon}_{ig} \;,
\]
where $\bm{X}$ is a $n\times p$ matrix specifying the value of each predictor and each cell;
$\bm{\beta}_g$ is a vector of coefficients, one for each predictor ($p$ in total);
and $\bm{\epsilon}_{g}$ is a vector of random variables $\epsilon_{ig}$, representing the error for each observation.
We assume that 
\[
    \epsilon_{ig} \sim \mbox{Normal}(0, \sigma^2_g)
\]
and is independently sampled for each observation.
We also write
\[
\bm{\mu}_g = \bm{X}\bm{\beta}_g
\]
where $\bm{\mu}_g$ is a vector of length $n$ containing mean expression values $\mu_{ig} = E(y_{ig})$.

For each gene, we fit a linear model to $\bm{y}_g$ using standard least-squares methods.
In the fitted model, denote $\hat\mu_{ig}$ as the fitted value for each observation, with
\[
\bm{\hat\mu}_{g} = \bm{X} \bm{\hat\beta}_g
\]
where $\bm{\hat\beta}_g$ contains gene-specific estimates for the coefficients.
We estimate $\sigma^2_g$ with
\[
\hat\sigma^2_g = \frac{\sum_{i=1}^{n} (y_{ig} - \hat\mu_{ig})^2}{n-p} \;.
\] 
In this manner, we obtain a variance estimate for each gene in the data set.
We also obtain an average expression value $A_g$ for each gene by averaging $y_{ig}$ over all $i$.

\section{Modelling the distribution of variance estimates}
Consider a set of ``control'' genes that are \textit{not} highly variable. 
Most commonly, each control gene $s$ is a spike-in transcript that has been added at the same quantity to each cell, though other definitions are possible.
As $\epsilon_{is}$ are normally distributed, we have
\[
    \hat\sigma^2_s|\sigma^2_s \sim \frac{\sigma^2_s\chi^2_{n-p}}{n-p} \;.
\]
Further assume that $\sigma^2_s$ are distributed around a mean-variance trend $V(.)$ as
\[
\sigma^2_s \sim V(A_s) d_0 \chi^{-2}_{d_0}
\]
where $d_0$ is the prior degrees of freedom \citep{smyth2004linear}.
This quantifies the variance in $\sigma^2_s$ due to gene-specific factors such as GC content or length or secondary structure affecting capture efficiency or amplification noise.
From these two expressions, we obtain
\begin{equation}
    \hat\sigma^2_s \sim V(A_s)F(n-p, d_0) \label{eqn:estdist}
\end{equation}
which allows us to estimate $d_0$ and $V(.)$ from the observed distribution of $\hat\sigma^2_s$.

\section{Estimating the distribution parameters}

\subsection{Overview}
To estimate the parameters of the $\sigma^2_s$ distribution, we fit a trend $f(.)$ to $\log(\hat\sigma^2_s)$ against $A_s$.
We use log-transformed values as $\exp[f(.)]$ guarantees positive values for $V(.)$.
The fitted value of the trend for each $s$ is an estimate of the expectation of the log-values for genes with similar $A_s$, denoted as
\[
    f(A_s) \approx E_{|A_s}[\log(\hat\sigma^2_s)] = \log[V(A_s)] + E\{\log[F(n-p, d_0)]\} \;.
\]
Dividing $\hat\sigma^2_s$ by $\exp[f(A_s)]$ yields a scaled F-distribution independent of $A_s$, i.e.,
\[
    \frac{\hat\sigma^2_s}{\exp[f(A_s)]} \sim \Phi F(n-p, d_0) \;,
\]
where $\Phi = \exp(E\{\log[F(n-p, d_0)]\})^{-1}$. 
Both $d_0$ and $\Phi$ can therefore be determined by robustly fitting an F-distribution to the above ratios \citep{phipson2016robust}, using the \texttt{fitFDistRobustly} function.
Multiplication of $\exp[f(.)]$ with $\Phi$ then yields an estimate of $V(.)$.

\subsection{Prefitting with a parametric curve}
In practice, we use a two-step procedure to fit $f(.)$.
The first step uses a parametric curve that models the distinct shape of the mean-variance relationship for log-expression values in scRNA-seq data.
This curve in the variance is defined with respect to the mean $x$ with the function
\begin{equation}
    f_1(x) = \frac{A x}{x^M + B} \label{eqn:paramtrend}
\end{equation}
with $A > 0$, $M > 1$ and $B > 0$.
We chose this function as it captures major features of the mean-variance trend including a linear increase from zero at low $x$, a smooth peak at moderate $x$ and an asymptotic decrease to zero as $x \to \infty$.

We use non-linear least-squares optimization to fit $f_1(.)$ to $\hat\sigma^2_s$ against $A_s$.
Sensible starting values for each parameter are obtained as follows:
\begin{enumerate}
\item The approximate location ($\hat{x}_m$) and height ($\hat{y}_m$) of the peak in the curve is obtained by fitting a loess smoother to $\sigma^2_s$.
\item The initial gradient $\hat{g}$ is obtained by fitting a line to the variances for spike-ins where $A_s$ is between $\hat{x}_m/2$ and the minimum $A_s$ across all spike-ins.
The use of this subset of spike-ins avoids the non-linearity as the curve approaches the peak.
\item Near zero, Equation~\ref{eqn:paramtrend} becomes $Ax/B$ when $M>1$ and $B>0$. 
Thus, if $B$ is known, $A$ can be computed by multiplying $B$ with $\hat{g}$.
\item The stationary point at Equation~\ref{eqn:paramtrend} occurs when $x^M = B/(M-1)$.
This allows us to solve for $B$ when $M$ is known, by replacing $x$ with our estimate of the peak location $\hat{x}_m$.
\item In theory, the height of the peak could be used to solve for $M$. 
However, this seems unstable in practice, so instead a grid search is used to identify the $M$ (and subsequently the $B$, and therefore the $A$) that minimizes the sum of squares.
\end{enumerate}
The resulting values of $M$, $A$ and $B$ are used in the \texttt{nls} function to refine the curve fit.
Searching is performed in the log-space to ensure that the constraints on $A$, $M$ and $B$ are respected.

In the second step, we remove most of the effect of the mean-variance curve by computing 
\[
    r_s = \hat\sigma^2_s/f_1(A_s) \;.
\]
We then fit a robust loess curve $f_2(.)$ to $\log(r_s)$ against $A_s$, and define
\[
    f(A_s) = \log[f_1(A_s)] + f_2(A_s)
\]
The second fit accounts for empirical variations from the relationship in Equation~\ref{eqn:paramtrend} and reduces the effect of any outliers.
It also ensures that the fit is ultimately determined by the sum of squares in the log-space, consistent with the original definition of $f(.)$ fitted to the log-variances.
The use of a two-step protocol is necessary because a complex trend with strong gradients is difficult to fit directly with smoothing algorithms.
By first using a parametric curve to model the complex parts of the trend, we can factor out the strong gradients to improve the accuracy of the loess smoothing.

\section{Modelling the technical noise for endogenous genes}
Consider an endogenous gene $g$, which has a true expression of $y_{ig}^*$ in cell $i$.
Let $\mbox{var}(y_{ig}^*) = \sigma^2_{(b)g}$, representing the underlying biological variation.
The observed expression $y_{ig}$ is then sampled from $y_{ig}|y_{ig}^* \sim T_{ig}$ with $E(T_{ig}) = y^*_{ig}$, representing the effect of technical noise.
The observed variance is 
\begin{align*}
    \sigma^2_g &= \mbox{var}[E(y_{ig}|y_{ig}^*)] + E[\mbox{var}(y_{ig}|y_{ig}^*)] \\
               &= \sigma^2_{(b)g} +  E[\mbox{var}(y_{ig}|y_{ig}^*)]  \;.
\end{align*}
Denote $E[\mbox{var}(y_{ig}|y_{ig}^*)]$ as $\sigma^2_{(t)g}$.
For a gene with abundance $A_g$, we estimate $\sigma^2_{(t)g}$ from the corresponding control genes.
Specifically, we use the expectation of $\sigma^2_s$ at this abundance (Equation~\ref{eqn:estdist}), i.e., 
\begin{equation}
    \hat\sigma^2_{(t)g} \approx E_{|A_g}[\sigma^2_s] = \frac{d_0V(A_g)}{d_0-2} \;. \label{eqn:techest}
\end{equation}
We can simply obtain an estimate of the biological component as
\[
    \hat\sigma^2_{(b)g} = \hat\sigma^2_g - \hat\sigma^2_{(t)g} \;,
\]
allowing the observed variance to be decomposed to its biological and technical components.

Using the variance of the control genes assumes that, at the very least, $\mbox{var}(y_{ig}|y_{ig}^*) = \mbox{var}(y_{is})$ when $E(y_{is}) = y_{ig}^*$, i.e., the sampling noise is the same for control and endogenous genes at the same expression level.
The quality of the approximation in Equation~\ref{eqn:techest} also depends on how $\mbox{var}(y_{ig}|y_{ig}^*)$ changes as a function of $y^*_{ig}$.
If it does not change, the expression is exact; however, this unlikely to be the case as the sampling noise for counts will depend on the mean.
The approximation is less likely to be accurate if there are strong changes with respect to $y_{ig}^*$ \textit{and} the variance of $y_{ig}^*$ across cells is high.
Nonetheless, this approach is still a useful approximation for modelling technical noise.

Finally, to detect highly variable genes, our null hypothesis is that all endogenous genes have variance estimates that are distributed according to Equation~\ref{eqn:estdist}.
Highly variable genes are detected by rejecting this null hypothesis with a one-sided $F$-test.
For an endogenous gene $g$, we compute
\[
    F_g = \frac{\hat\sigma^2_g}{V(A_g)} \;.
\]
The $p$-value for this gene is defined as the upper tail probability at $F_g$ for an $F$-distribution with $n-p$ and $d_0$ degrees of freedom.

\section{Further comments}

\subsection{Filtering out low-abundance genes}
At very low abundances, counts are limited to values of 0 or 1.
This means that the log-expression values effectively follow a scaled binomial distribution, for which the relationship between the sample mean and variance estimate is exact.
In other words, all low-abundance genes lie exactly on the empirical mean-variance trend with no scatter.
When fitting $f(.)$, this understates the variance around the trend and leads to overestimation of $d_0$ for all other genes.
The preponderance of low-abundance genes also interferes with span-based loess smoothing at higher abundances.
Thus, filtering of these genes should be performed prior to trend fitting.
The simulations in \texttt{simulations/sim\_filter.R} indicate that a threshold of 0.1 on the $\log_2(y_{ig}+1)$ mean should be used.
Below this threshold, discreteness in the variance estimates begins to dominate.

\subsection{Avoiding distributional assumptions}
Currently, the above framework assumes normality for the log-expression values, which is probably unreasonable.
The trend fitting is unlikely to be strongly affected as normality only affects $\Phi$, for which the estimate of $d_0$ can change to accommodate any non-normality.
However, the $p$-value from the F-test is likely to be invalidated by strong non-normality.

A better approach would be to take advantage of the central limit theorem for the variance estimates themselves.
This would allow us to assume that $\hat\sigma^2_s|\sigma^2_s \sim \sigma^2_s N(1, \psi_0)$ for some sampling variance $\psi_0$.
The use of the CLT should be possible as the variance estimates are simply the sum of squared residuals.
(The lost degrees of freedom can effectively be ignored for large $n$ and small $p$.)
We can model $\sigma^2_s \sim V'(A_s)N(1, \psi_1)$ for a mean-variance trend $V'(.)$, where $\psi_1$ represents the spread of true variances (equivalent to $d_0$).
This would yield $\hat\sigma^2_s \sim V'(A_s)N(1, \psi_1 + \psi_0)$.

$V'(.)$ can be easily fitted using a two-step procedure similar to that described above.
One can imagine a parametric fit followed by a loess fit on the re-scaled variances.
A log-transformation is not useful in this case, as $\Phi$ is not defined for a log-transformed normal variate.
Fortunately, the rescaling should bring most values to unity, so negative fitted values should not be a concern.

The more relevant aspect is the hypothesis testing.
The null hypothesis for gene $g$ states that $\hat\sigma^2_g  \sim V'(A_g)N(1, \psi_1 + \psi_0)$, assuming that spike-ins and endogenous genes are comparable.
While a $z$-test is the most obvious strategy here, it is also possible to use a one-sided $t$-test to account for the uncertainty of estimating $\psi_1 + \psi_0$.
This seems to yield satisfactory type I error control in a variety of situations (see the results in \texttt{moresims/}).

\bibliography{ref}
\bibliographystyle{plainnat}

\end{document}
