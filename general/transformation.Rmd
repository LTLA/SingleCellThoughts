---
title: What transformation should we use?
author: Aaron Lun
date: "`r Sys.Date()`"
output:
  BiocStyle::html_document:
    toc: true
    toc_float: true
bibliography: ref.bib
---   

```{r, echo=FALSE, message=FALSE}
library(BiocStyle)
knitr::opts_chunk$set(error=FALSE, message=FALSE, warning=FALSE)
set.seed(120000)
```

# Background

Transformation of expression data aims to achieve variance stabilization, 
i.e., an identical noise distribution for all observations.
This simplifies downstream analyses by circumventing the need for complex models to account for the mean-variance relationship.
For example, one could apply linear models directly under the assumption of i.i.d. normal errors. 
In the context of scRNA-seq data analysis, the motivation for transformation is more nebulous.
Most common procedures are not model-based and do not assume i.i.d. errors in the first place.
Nonetheless, there are some benefits to using transformed data, which we will discuss below.

# Effects of transformation

## Implicit feature selection

One major effect of any transformation is that of implicit feature selection, or "feature weighting".
In untransformed data, genes with large counts drive population heterogeneity simply because they have higher variance.
Log-transformation downweights the contribution of these counts to high-dimensional procedures like clustering.
Other transformations do the same, only differing in the extent of the reweighting with respect to abundance.

Whether or not this effect is desirable is another question.
Large counts usually occur in constitutively expressed genes that are typically not useful for cell type identification,
so downweighting their contribution via a log-transformation is a standard operation in scRNA-seq data analysis.
However, if relevant expression differences occurred in these genes (e.g., ribosome synthesis is of interest),
it could conceivably be better to avoid transformation prior to clustering.

Following this reasoning, a true variance stabilizing transformation (VST) may not actually be optimal for feature weighting.
By equalizing the variance across all abundances, the VST will retain uninteresting variation - that is, noise - at large counts.
This may subsequently obscure more interesting aspects of population heterogeneity in the downstream analysis.
(Note that this is a function of the count, not the gene, so explicit feature selection does not necessarily help.)
In contrast, log-transformation aggressively downweights the variance of large counts in near-Poisson data,
reducing their downstream influence at the cost of retaining a mean-variance trend after transformation.

<!--
To elaborate on why feature selection may not help:
a gene can be highly variable and contain large counts, e.g., if a subpopulation highly upregulates the gene.
Such a gene would probably be selected in any feature selection strategy.
The problem arises when the variability in the upregulated subpopulation is solely driven by the magnitude of the large count,
such that further structure from other lower-abundance genes in that subpopulation is no longer apparent.
-->

The optimal transformation would be the one that gave greatest weight to the abundance interval containing the most relevant genes.
Obviously, this is either unknown or in the eye of the beholder.
One appeal of a true VST is that it would assume nothing about the choice of interval,
sacrificing optimal performance for some robustness across different data sets.
The log-transformation's choice of pseudo-count effectively tunes the most highly weighted interval,
with larger pseudo-counts favouring higher implicit weights for genes at higher abundances.

## Variable variances due to coverage

Variance stabilization ensures that the population variance is independent of the coverage of that population.
For example, a population with deeper sequencing/coverage would have lower variance in the normalized expression values,
which may be misleading for comparisons of heterogeneity (e.g., to study differentiation priming).
Similarly, a population that contains a spread of library sizes will exhibit some structure whereby high-coverage cells are enveloped by low-coverage cells.
A VST aims to remove such library size-dependent differences in variance,
which is only possible when the mean-abundance trend is eliminated.

Admittedly, differences in variance tend not to the primary focus of the analysis.
Of greater concern is the absence of any guarantee that the equality of expectations is preserved after transformation.
To illustrate, consider a situation where the normalized expression from cells of different coverage have equal expectations.
If this equality is not preserved in transformed data, the transformation will have re-introduced coverage-dependent structure.
This clearly occurs with the log-transformation [@lun2018overcoming],
and the second-order Taylor approximation suggests that it will occur to some extent for all transformations.
We discuss this in more detail [below](#preserving-equality-of-expectations).

## Other

Transformation can to improve the dynamic range of expression values and thus resolution of differences.
For example, in a mixture of three populations with mean counts of 1, 10 and 100, 
the first two are effectively indistinguishable without transformation.

Transformation can also simplify interpretation of downstream statistics.
In particular, distances in log-transformed data can be treated as an estimate of the log-fold change in the (normalized) counts.

# Practical challenges

## With low counts

No transformation can achieve complete variance stabilization in scRNA-seq data [@lun2018overcoming], especially UMI data.
At a mean of zero, the variance is necessarily zero.
At any non-zero mean, the variance can only increase from zero.
Thus, there is an inherent mean-variance relationship that cannot be removed by any (monotonic) transformation.
This is particularly relevant for UMI data due to the low counts and high frequency of zeroes.

Of course, the trend near zero can be minimized by using a transformation that narrows the interval to stabilization.
If a sufficiently narrow interval is achieved, very low-abundance genes with few non-zero counts will have the same contribution to population heterogeneity as genes with higher expression.
However, this is problematic given that Poisson noise dominates at such low counts.
Upweighting this technical noise would probably be counter-productive.

## Handling biological variation

### Example with Poisson noise

A transformation to stabilize noise will not necessarily remove the dependence on the mean for biological variation.
Consider some counts with Poisson-distributed noise. 
For the Poisson distribution, the variance stabilizing transformation (VST) is the square root function.
While this stabilizes the Poisson component at high means, any overdispersion will manifest as an increasing mean-variance trend:

```{r, fig.wide=TRUE}
library(matrixStats)
mu <- 2^seq(0, 10, length.out=100)
y <- matrix(rpois(length(mu)*1000, lambda=mu), nrow=length(mu))
v <- rowVars(sqrt(y))

y2 <- matrix(rnbinom(length(mu)*1000, mu=mu, size=10), nrow=length(mu))
v2 <- rowVars(sqrt(y2))

par(mfrow=c(1,2))
plot(mu, v, xlab="Mean", ylab="Variance", main="Poisson", log="x")
plot(mu, v2, xlab="Mean", ylab="Variance", main="Overdispersed", log="x")
```

This is unlikely to be desirable, as it means that high-abundance genes can dominate the downstream analyses.
Similarly, a two-fold change in expression has a different effect depending on the abundance:

```{r, fig.wide=TRUE}
y3 <- cbind(
    matrix(rpois(length(mu)*500, lambda=mu), nrow=length(mu)),
    matrix(rpois(length(mu)*500, lambda=mu*2), nrow=length(mu))
)
v3 <- rowVars(sqrt(y3))
vl <- rowVars(log(y3+1)) # log-transformation, for comparison.

par(mfrow=c(1,2))
plot(mu, v3, xlab="Mean", ylab="Variance", main="Overdispersed (sqrt)", log="x")
plot(mu, vl, xlab="Mean", ylab="Variance", main="Overdispersed (log)", log="x")
```

This is most obviously problematic for high-dimensional procedures that involve implicit comparisons between genes, e.g., clustering.
However, it will also compromise any comparisons across different means, e.g., log-fold change estimates, testing for interaction effects.

### Example with overdispersion

One might suggest that the choice of VST should instead be based on the mean-variance relationship after including the biological component.
This simply shifts the problem; any overdispersion above the trend would have different effects at different means.
Consider the following example:

```{r}
ngenes <- 10000
means <- 2^runif(ngenes, 0, 10)
dispersion <- rep(0.1, ngenes)
dispersion[sample(ngenes, 100)] <- 1

ncells <- 500
counts <- matrix(rnbinom(ngenes*ncells, mu=means, 
    size=1/dispersion), ncol=ncells)
rownames(counts) <- sprintf("GENE_%i", seq_len(ngenes))
colnames(counts) <- sprintf("CELL_%i", seq_len(ncells))
```

We apply VST functions from `r Biocpkg("DESeq2")` and `r CRANpkg("sctransform")`.
The former fits a mean-dispersion trend and uses it to derive an appropriate VST [@anders2010differential],
while the latter package uses Pearson residuals from a NB GLM as the transformed values [@hafemeister2019normalization].
In both cases, the VST fails to stabilize the biological overdispersion above the trend. 

```{r}
outDS <- DESeq2::vst(counts)
outSC <- sctransform::vst(counts, show_progress=FALSE)

par(mfrow=c(1,2))
plot(means, matrixStats::rowVars(outDS), log="xy", 
    main="DESeq2", col=(dispersion > 0.5) + 1)
plot(means, matrixStats::rowVars(outSC$y), log="xy", 
    main="sctransform", col=(dispersion > 0.5)+1)
```

We provide another example using constant log-fold changes.
This simulation occurs at higher abundances but with a stronger mean-dispersion trend to introduce some complexity.

```{r}
set.seed(1000)
ngenes <- 10000
mu <- 2^runif(ngenes, 5, 10)
dispersion <- 10/mu + 0.1
ncells <- 200
means <- matrix(mu, nrow=ngenes, ncol=ncells)

# Balanced DE that does not change library size or average abundance.
first <- 1:100
second <- first + 100
g1 <- 1:100
g2 <- 100+g1
means[first,g1] <- means[first,g1]*2
means[first,g2] <- means[first,g2]/2
means[second,g1] <- means[second,g1]/2
means[second,g2] <- means[second,g2]*2

counts <- matrix(rnbinom(ngenes*ncells, mu=means, 
    size=1/dispersion), ncol=ncells)
rownames(counts) <- sprintf("GENE_%i", seq_len(ngenes))
colnames(counts) <- sprintf("CELL_%i", seq_len(ncells))
```

While the effect is weaker, we again see that the mean-variance trend is not stabilized for the overdispersed genes.
This phenomenon means that the high-abundance genes are still implicitly upweighted after each VST.
Whether this trend actually causes problems is debatable - but if it doesn't,
it undermines the argument for careful variance stabilization in the first place.

```{r}
outDS <- DESeq2::vst(counts)
outSC <- sctransform::vst(counts, show_progress=FALSE)
col <- rep("black", ngenes)
col[c(first, second)] <- "red"

par(mfrow=c(1,2))
plot(mu, matrixStats::rowVars(outDS), log="xy", 
    main="DESeq2", col=col)
plot(mu, matrixStats::rowVars(outSC$y), log="xy", 
    main="sctransform", col=col)
```

<!--
Using `vst` and a simple log-transformation in this simulation indicates that only the latter preserves the size of the log-fold change across abundances.

```{r}
plot(mu[first], (rowMeans(outDS[first,g1]) - rowMeans(outDS[first,g2])))
outL <- log2(counts + 1)    
plot(mu[first], (rowMeans(outL[first,g1]) - rowMeans(outL[first,g2])))
```

The counts are pretty large here, so the value of the pseudo-count should not be an issue, nor should there be any shrinkage to compensate for Poisson noise.
Rather, this is due to the fact that `vst` automatically squeezes values together more strongly when the count data are noisy, in order to stabilize the variance.
Less squeezing occurs at higher counts, resulting in a distortion of the log-fold changes (equivalent to that of the biological component).
-->

## Preserving equality of expectations

Consider a situation with libaries of different sizes.
This should have no structure after library size normalization.

```{r}
ngenes <- 10000
mu <- 2^runif(ngenes, 0, 10)
libsize <- rep(rep(c(0.5, 2), each=50), 2)
means <- outer(mu, libsize)

counts <- matrix(rnbinom(ngenes*ncol(means), mu=means, size=100), 
    ncol=ncol(means))
rownames(counts) <- sprintf("GENE_%i", seq_len(ngenes))
colnames(counts) <- sprintf("CELL_%i", seq_len(ncells))
```

We can see that both VSTs fail to fully preserve the equality of expected normalized expression values,
with some structure correlated to the library size.

```{r}
outDS <- DESeq2::vst(counts)
outSC <- sctransform::vst(counts, show_progress=FALSE)

pcDS <- prcomp(t(outDS))
pc.sc <- prcomp(t(outSC$y))

par(mfrow=c(1,2))
plot(pcDS$x[,1], pcDS$x[,2], col=(libsize > 1)+1,
    main="DESeq2", xlab="PC1", ylab="PC2")
plot(pc.sc$x[,1], pc.sc$x[,2], col=(libsize > 1)+1,
    main="sctransform", xlab="PC1", ylab="PC2")
```

`r CRANpkg("sctransform")` seems more performant in this example, with weaker structure associated with the library size.
However, the advantage disappears when one introduces more complexity.
The simulation below separates the cells into two populations, where each population contains a variety of libary sizes.
After normalization, we should see only separation between populations.

```{r}
means <- outer(mu, libsize)

first <- 1:100
second <- first + 100
g1 <- 1:100
g2 <- 100+g1
means[first,g1] <- means[first,g1]*2
means[first,g2] <- means[first,g2]/2
means[second,g1] <- means[second,g1]/2
means[second,g2] <- means[second,g2]*2

counts <- matrix(rnbinom(ngenes*ncol(means), mu=means, size=100), 
    ncol=ncol(means))
rownames(counts) <- sprintf("GENE_%i", seq_len(ngenes))
colnames(counts) <- sprintf("CELL_%i", seq_len(ncells))
```

We see that both VSTs now yield separate structures according to the library size.
This serves to demonstrate that the equality of expectations is not guaranteed to be preserved after transformation.
In fact, `r CRANpkg("sctransform")` manages to invert the positions of the size-related structure, which is rather unexpected.

```{r}
outDS <- DESeq2::vst(counts)
outSC <- sctransform::vst(counts, show_progress=FALSE)

pcDS <- prcomp(t(outDS))
pcSC <- prcomp(t(outSC$y))

par(mfrow=c(1,2))
plot(pcDS$x[,1], pcDS$x[,2], col=(libsize > 1)+1,
    main="DESeq2", xlab="PC1", ylab="PC2")
plot(pcSC$x[,1], pcSC$x[,2], col=(libsize > 1)+1,
    main="sctransform", xlab="PC1", ylab="PC2")
```

Of course, this example is somewhat pathological.
We might actually consider consistent differences in library sizes to be a valid indicator of a distinct subpopulation,
such that the structure observed above is actually biologically useful.
In practice, we would expect to see a smooth distribution of library sizes due to technical variation in coverage.
This means that any library size-related structure induced by transformation would more likely form spurious trajectories - 
which, arguably, can be just as problematic.

# Miscellaneous comments

The use of the log-total count as a covariate in the GLM fit of `r CRANpkg("sctransform")` is somewhat concerning.
However, it is protected by using the regularized (i.e., abundance-trended) estimates of all coefficients.
This assumes that most genes are not correlated at a given abundance, equivalent to the assumption of `r Biocpkg("SCnorm")`.
It is likely that this is a reasonable assumption in the majority of cases.
Nonetheless, some biological variation will be regressed out in the case where this assumption fails.

```{r, fig.wide=TRUE}
ngenes <- 10000
means <- 2^sort(runif(ngenes, 0, 10))
ncells <- 500
counts <- matrix(rnbinom(ngenes*ncells, mu=means, size=10), ncol=ncells)

# Some structure (imperfectly) correlated with library size,
# concentrated around the mean of mean(1:500).
N <- 1000
for (i in 1:N) { 
    counts[i,] <- rnbinom(ncells, mu=1:500, size=10)
}
rownames(counts) <- sprintf("GENE_%i", seq_len(ngenes))
colnames(counts) <- sprintf("CELL_%i", seq_len(ncells))

outSC <- sctransform::vst(counts, show_progress=FALSE)
normed <- t(t(counts)/scater::librarySizeFactors(counts))
plot(
    matrixStats::rowVars(log(normed+1)),
    matrixStats::rowVars(outSC$y),
    col=1+as.integer(seq_len(ngenes) <= N),
    xlab="Log expression variance",
    ylab="scTransform variance"
)
```

The transformed values also exhibit no relation to the original scale of the (log-)counts.
This is not a problem for exploratory analyses but makes it difficult to interpret differential expression analyses.
`r Biocpkg("DESeq2")`'s VST converges to the log-fold change at high abundances but it is not entirely faultless;
a strong mean-dispersion trend is sufficient to induce major discrepancies, 
even when the counts are sufficiently large to obtain an accurate log-fold change estimate.

```{r}
ngenes <- 10000
means <- 2^sort(runif(ngenes, 0, 10))
ncells <- 500

counts <- matrix(rnbinom(ngenes*ncells, mu=means, size=10), ncol=ncells)
rownames(counts) <- sprintf("GENE_%i", seq_len(ngenes))
colnames(counts) <- sprintf("CELL_%i", seq_len(ncells))

outSC <- sctransform::vst(counts, show_progress=FALSE)
smoothScatter(matrixStats::rowVars(log2(counts+1)),
    matrixStats::rowVars(outSC$y))
```

A major practical issue with both VSTs is that they are not sparsity preserving.
This affects the computational performance of all downstream analyses that are performed on the transformed data.

# Concluding remarks

In most cases, the log-transformation is probably satisfactory.
It is fast, simple to compute and allows interpretation of differences as log-fold changes (at high abundances, at least).
It implicitly upweights genes of moderate abundance that are most likely to be biologically informative.
The choice of pseudo-count also provides a tuning parameter for abundance-dependent weighting during transformation.

# Session information

```{r}
sessionInfo()
```

# References


