---
title: What transformation should we use?
author: Aaron Lun
date: "`r Sys.Date()`"
output:
  BiocStyle::html_document:
    toc: true
    toc_float: true
bibliography: ref.bib
---   

```{r, echo=FALSE, message=FALSE}
library(BiocStyle)
knitr::opts_chunk$set(error=FALSE, message=FALSE, warning=FALSE)
set.seed(120000)
```

# Background

Transformation of expression data aims to achieve variance stabilization, 
i.e., an identical noise distribution for all observations.
This simplifies downstream analyses by circumventing the need for complex models to account for the mean-variance relationship.
For example, one could apply linear models directly under the assumption of i.i.d. normal errors. 
In the context of scRNA-seq data analysis, the motivation for transformation is more nebulous.
Most common procedures are not model-based and do not assume i.i.d. errors in the first place.
Nonetheless, there are some benefits to using transformed data, which we will discuss below.

# Effects of variance stabilization

## Implicit feature selection

One major effect of variance stabilization is that of implicit feature selection, or "feature weighting".
In untransformed data, high-abundance genes would drive population heterogeneity simply because they have higher variance.
Log-transformation downweights the contribution of those genes to high-dimensional procedures like clustering.
Other transformations do the same task, only differing in the extent of the reweighting with respect to abundance.

Whether or not this effect is desirable is another question.
High-abundance genes are dominated by constitutively expressed genes that are typically not useful for cell type identification,
so downweighting their contribution to the variance is a fairly standard operation.
However, if relevant expression differences occurred in high-abundance genes (e.g., ribosome synthesis is of interest),
it would be better to avoid transformation prior to procedures such as clustering.

Assuming that the reweighting is desirable, we now need to consider which transformation yields the optimum weights.
In this respect, a true variance stabilizing transformation may be less performant than a simple log-transformation.
This is because the log-transformation strongly downweights the contribution from (presumed uninteresting) high-abundance genes.
Increasing their weight with a VST will re-introduce unnecessary noise into the downstream analysis.

The optimal transformation would be the one that gave greatest weight to the abundance interval containing the most relevant genes.
Generally speaking, this refers to genes that have only moderate-to-low average expression across the data set.
This is because they are likely to be upregulated in some cells and downregulated in others,
and thus be associated with biological heterogeneity within the population.

Of course, the actual relevance of a gene is either unknown or in the eye of the beholder.
The log-transformation's choice of pseudo-count allows users to tune the most highly weighted interval,
with larger pseudo-counts favouring higher implicit weights for genes at higher abundances.
One appeal of a true VST is that it would assume nothing about the choice of interval,
sacrificing optimal performance for some robustness across different data sets.

## Handling library size effects

Variance stabilization ensures that the population variance is independent of the coverage of that population.
For example, a population with deeper sequencing/coverage would have lower variance in the normalized expression values,
which may be misleading for comparisons of heterogeneity (e.g., to study differentiation priming).
Similarly, a population that contains a spread of library sizes will exhibit some structure whereby high-coverage cells are enveloped by low-coverage cells.
A VST aims to remove such library size-dependent differences in variance,
which is only possible when the mean-abundance trend is eliminated.

Admittedly, differences in variance tend not to the primary focus of the analysis.
Of greater concern is the absence of any guarantee that the equality of expectations is preserved after transformation.
To illustrate, consider a situation where the normalized expression from cells of different coverage have equal expectations.
If this equality is not preserved in transformed data, the transformation will have re-introduced coverage-dependent structure.
This clearly occurs with the log-transformation [@lun2018overcoming],
and the second-order Taylor approximation suggests that it will occur to some extent for all transformations.

## Other

Transformation can to improve the dynamic range of expression values and thus resolution of differences.
For example, in a mixture of three populations with mean counts of 1, 10 and 100, 
the first two are effectively indistinguishable without transformation.

Transformation can also simplify interpretation of downstream statistics.
In particular, distances in log-transformed data can be treated as an estimate of the log-fold change in the (normalized) counts.

# Practical challenges

## With low counts

No transformation can achieve complete variance stabilization in scRNA-seq data [@lun2018overcoming], especially UMI data.
At a mean of zero, the variance is necessarily zero.
At any non-zero mean, the variance can only increase from zero.
Thus, there is an inherent mean-variance relationship that cannot be removed by any (monotonic) transformation.
This is particularly relevant for UMI data due to the low counts and high frequency of zeroes.

Of course, the trend near zero can be minimized by using a transformation that narrows the interval to stabilization.
If a sufficiently narrow interval is achieved, very low-abundance genes with few non-zero counts will have the same contribution to population heterogeneity as genes with higher expression.
However, this is problematic given that Poisson noise dominates at such low counts.
Upweighting this technical noise would be largely counter-productive.

## Handling biological variation

### Example with Poisson noise

A transformation to stabilize noise will not necessarily remove the dependence on the mean for biological variation.
Consider some counts with Poisson-distributed noise. 
For the Poisson distribution, the variance stabilizing transformation (VST) is the square root function.
While this stabilizes the Poisson component at high means, any overdispersion will manifest as an increasing mean-variance trend:

```{r, fig.wide=TRUE}
library(matrixStats)
mu <- 2^seq(0, 10, length.out=100)
y <- matrix(rpois(length(mu)*1000, lambda=mu), nrow=length(mu))
v <- rowVars(sqrt(y))

y2 <- matrix(rnbinom(length(mu)*1000, mu=mu, size=10), nrow=length(mu))
v2 <- rowVars(sqrt(y2))

par(mfrow=c(1,2))
plot(mu, v, xlab="Mean", ylab="Variance", main="Poisson", log="x")
plot(mu, v2, xlab="Mean", ylab="Variance", main="Overdispersed", log="x")
```

This is unlikely to be desirable, as it means that high-abundance genes can dominate the downstream analyses.
Similarly, a two-fold change in expression has a different effect depending on the abundance:

```{r, fig.wide=TRUE}
y3 <- cbind(
    matrix(rpois(length(mu)*500, lambda=mu), nrow=length(mu)),
    matrix(rpois(length(mu)*500, lambda=mu*2), nrow=length(mu))
)
v3 <- rowVars(sqrt(y3))
vl <- rowVars(log(y3+1)) # log-transformation, for comparison.

par(mfrow=c(1,2))
plot(mu, v3, xlab="Mean", ylab="Variance", main="Overdispersed (sqrt)", log="x")
plot(mu, vl, xlab="Mean", ylab="Variance", main="Overdispersed (log)", log="x")
```

This is most obviously problematic for high-dimensional procedures that involve implicit comparisons between genes, e.g., clustering.
However, it will also compromise any comparisons across different means, e.g., log-fold change estimates, testing for interaction effects.

### Example with overdispersion

One might suggest that the choice of VST should instead be based on the mean-variance relationship after including the biological component.
This simply shifts the problem; any overdispersion above the trend would have different effects at different means.
Consider the following example:

```{r}
ngenes <- 10000
means <- 2^runif(ngenes, 0, 10)
dispersion <- rep(0.1, ngenes)
dispersion[sample(ngenes, 1000)] <- 1

ncells <- 500
counts <- matrix(rnbinom(ngenes*ncells, mu=means, 
    size=1/dispersion), ncol=ncells)
rownames(counts) <- sprintf("GENE_%i", seq_len(ngenes))
colnames(counts) <- sprintf("CELL_%i", seq_len(ncells))
```

We apply VST functions from `r Biocpkg("DESeq2")` and `r CRANpkg("sctransform")`.
The former fits a mean-dispersion trend and uses it to derive an appropriate VST,
while the latter package uses Pearson residuals as the transformed values [@hafemeister2019normalization].
`r CRANpkg("sctransform")`'s strategy is not dissimilar to the `zscoreNBinom` function from `r Biocpkg("edgeR")`, 
which does a similar conversion of count data to a continuous space based on the NB distribution.
However, in both cases, the VST fails to stabilize the biological overdispersion above the trend. 

```{r}
outDS <- DESeq2::vst(counts)
outSC <- sctransform::vst(counts, show_progress=FALSE)

par(mfrow=c(1,2))
plot(means, matrixStats::rowVars(outDS), log="xy", 
    main="DESeq2", col=(dispersion > 0.5) + 1)
plot(means, matrixStats::rowVars(outSC$y), log="xy", 
    main="sctransform", col=(dispersion > 0.5)+1)
```

We provide another example using constant log-fold changes.
This simulation occurs at higher abundances but with a stronger mean-dispersion trend to introduce some complexity.

```{r}
set.seed(1000)
ngenes <- 10000
mu <- 2^runif(ngenes, 5, 10)
dispersion <- 10/mu + 0.1
ncells <- 200
means <- matrix(mu, nrow=ngenes, ncol=ncells)

# Balanced DE that does not change library size or average abundance.
first <- 1:1000
second <- first + 1000
g1 <- 1:100
g2 <- 100+g1
means[first,g1] <- means[first,g1]*2
means[first,g2] <- means[first,g2]/2
means[second,g1] <- means[second,g1]/2
means[second,g2] <- means[second,g2]*2

counts <- matrix(rnbinom(ngenes*ncells, mu=means, 
    size=1/dispersion), ncol=ncells)
rownames(counts) <- sprintf("GENE_%i", seq_len(ngenes))
colnames(counts) <- sprintf("CELL_%i", seq_len(ncells))
```

While the effect is weaker, we again see that the mean-variance trend is not stabilized for the overdispersed genes.

```{r}
outDS <- DESeq2::vst(counts)
outSC <- sctransform::vst(counts, show_progress=FALSE)
col <- rep("black", ngenes)
col[c(first, second)] <- "red"

par(mfrow=c(1,2))
plot(mu, matrixStats::rowVars(outDS), log="xy", 
    main="DESeq2", col=col)
plot(mu, matrixStats::rowVars(outSC$y), log="xy", 
    main="sctransform", col=col)
```

This phenomenon means that the high-abundance genes are still implicitly upweighted after transformation.
Whether this actually causes problems is debatable, but it certainly weakens the argument for using a VST in the first place.

<!--
Using `vst` and a simple log-transformation in this simulation indicates that only the latter preserves the size of the log-fold change across abundances.

```{r}
plot(mu[first], (rowMeans(outDS[first,g1]) - rowMeans(outDS[first,g2])))
outL <- log2(counts + 1)    
plot(mu[first], (rowMeans(outL[first,g1]) - rowMeans(outL[first,g2])))
```

The counts are pretty large here, so the value of the pseudo-count should not be an issue, nor should there be any shrinkage to compensate for Poisson noise.
Rather, this is due to the fact that `vst` automatically squeezes values together more strongly when the count data are noisy, in order to stabilize the variance.
Less squeezing occurs at higher counts, resulting in a distortion of the log-fold changes (equivalent to that of the biological component).
-->

## Preserving equality of expectations

Consider a situation with libaries of different sizes.
This should have no structure after library size normalization.

```{r}
ngenes <- 10000
mu <- 2^runif(ngenes, 0, 10)
libsize <- rep(rep(c(0.5, 2), each=50), 2)
means <- outer(mu, libsize)

counts <- matrix(rnbinom(ngenes*ncol(means), mu=means, size=100), 
    ncol=ncol(means))
rownames(counts) <- sprintf("GENE_%i", seq_len(ngenes))
colnames(counts) <- sprintf("CELL_%i", seq_len(ncells))
```

We can see that both VSTs fail to fully preserve the equality of expected normalized expression values,
with some structure correlated to the library size.

```{r}
outDS <- DESeq2::vst(counts)
outSC <- sctransform::vst(counts, show_progress=FALSE)

pcDS <- prcomp(t(outDS))
pc.sc <- prcomp(t(outSC$y))

par(mfrow=c(1,2))
plot(pcDS$x[,1], pcDS$x[,2], col=(libsize > 1)+1,
    main="DESeq2", xlab="PC1", ylab="PC2")
plot(pc.sc$x[,1], pc.sc$x[,2], col=(libsize > 1)+1,
    main="sctransform", xlab="PC1", ylab="PC2")
```

`r CRANpkg("sctransform")` seems more performant in this example, with weaker structure associated with the library size.
However, the advantage disappears when one introduces more complexity.
The simulation below separates the cells into two populations, where each population contains a variety of libary sizes.
After normalization, we should see only separation between populations.

```{r}
means <- outer(mu, libsize)

first <- 1:1000
second <- first + 1000
g1 <- 1:100
g2 <- 100+g1
means[first,g1] <- means[first,g1]*2
means[first,g2] <- means[first,g2]/2
means[second,g1] <- means[second,g1]/2
means[second,g2] <- means[second,g2]*2

counts <- matrix(rnbinom(ngenes*ncol(means), mu=means, size=100), 
    ncol=ncol(means))
rownames(counts) <- sprintf("GENE_%i", seq_len(ngenes))
colnames(counts) <- sprintf("CELL_%i", seq_len(ncells))
```

We see that both VSTs now yield separate structures according to the library size.
This serves to demonstrate that the equality of expectations is not guaranteed to be preserved after transformation.
In fact, `r CRANpkg("sctransform")` manages to invert the positions of the size-related structure, which is rather unexpected.

```{r}
outDS <- DESeq2::vst(counts)
outSC <- sctransform::vst(counts, show_progress=FALSE)

pcDS <- prcomp(t(outDS))
pcSC <- prcomp(t(outSC$y))

par(mfrow=c(1,2))
plot(pcDS$x[,1], pcDS$x[,2], col=(libsize > 1)+1,
    main="DESeq2", xlab="PC1", ylab="PC2")
plot(pcSC$x[,1], pcSC$x[,2], col=(libsize > 1)+1,
    main="sctransform", xlab="PC1", ylab="PC2")
```

Of course, this example is somewhat pathological.
We might actually consider consistent differences in library sizes to be a valid indicator of a distinct subpopulation,
such that the structure observed above is actually biologically useful.
In practice, we would expect to see a smooth distribution of library sizes due to technical variation in coverage.
This means that any library size-related structure induced by transformation would more likely form spurious trajectories - 
which, arguably, can be just as problematic.

# Miscellaneous comments

## Dissecting `scTransform`

The use of the log-total count as a covariate in the GLM fit of `r CRANpkg("sctransform")` is somewhat concerning.
However, it is protected by using the regularized (i.e., abundance-trended) estimates of all coefficients.
This assumes that most genes are not correlated at a given abundance, equivalent to the assumption of `r Biocpkg("SCnorm")`.
It is likely that this is a reasonable assumption in the majority of cases.
Nonetheless, some biological variation will be regressed out in the case where this assumption fails.

```{r, fig.wide=TRUE}
ngenes <- 10000
means <- 2^sort(runif(ngenes, 0, 10))
ncells <- 500
counts <- matrix(rnbinom(ngenes*ncells, mu=means, size=10), ncol=ncells)

# Some structure (imperfectly) correlated with library size,
# concentrated around the mean of mean(1:500).
N <- 1000
for (i in 1:N) { 
    counts[i,] <- rnbinom(ncells, mu=1:500, size=10)
}
rownames(counts) <- sprintf("GENE_%i", seq_len(ngenes))
colnames(counts) <- sprintf("CELL_%i", seq_len(ncells))

outSC <- sctransform::vst(counts, show_progress=FALSE)
normed <- t(t(counts)/scater::librarySizeFactors(counts))
plot(
    matrixStats::rowVars(log(normed+1)),
    matrixStats::rowVars(outSC$y),
    col=1+as.integer(seq_len(ngenes) <= N),
    xlab="Log expression variance",
    ylab="scTransform variance"
)
```

The transformed values also exhibit no relation to the original scale of the (log-)counts.
This is not a problem for exploratory analyses but makes it difficult to interpret differential expression analyses.

```{r}
ngenes <- 10000
means <- 2^sort(runif(ngenes, 0, 10))
ncells <- 500

counts <- matrix(rnbinom(ngenes*ncells, mu=means, size=10), ncol=ncells)
rownames(counts) <- sprintf("GENE_%i", seq_len(ngenes))
colnames(counts) <- sprintf("CELL_%i", seq_len(ncells))

outSC <- sctransform::vst(counts, show_progress=FALSE)
smoothScatter(matrixStats::rowVars(log2(counts+1)),
    matrixStats::rowVars(outSC$y))
```

# Concluding remarks

In most cases, the log-transformation is probably satisfactory.
It is fast, simple to compute and allows interpretation of differences as log-fold changes (at high abundances, at least).
It implicitly upweights genes of moderate abundance that are most likely to be biologically informative.
The choice of pseudo-count also provides a tuning parameter for abundance-dependent weighting during transformation.

# Session information

```{r}
sessionInfo()
```

# References


