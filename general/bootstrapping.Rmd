---
title: Bootstrapping for cluster stability
author: Aaron Lun
date: "`r Sys.Date()`"
output:
  BiocStyle::html_document:
    toc: true
    toc_float: true
bibliography: ref.bib
---
    
```{r, echo=FALSE, results="hide"}
knitr::opts_chunk$set(error=FALSE, warning=FALSE, message=FALSE)
set.seed(100)
```

# Overview

Exploratory analyses of single-cell RNA sequencing (scRNA-seq) data often involve clustering to summarize the data for further interpretation.
It is routine to assess the quality of the clustering, e.g., based on how separated or modular the clusters are.
To this end, the concept of "cluster stability" is used to determine whether the clusters are reproducible in a replicate experiment.
Bootstrapping is often used for this purpose, providing a non-parametric method to generate replicate datasets _in silico_.
Here we will discuss some of the common uses of bootstrapping in scRNA-seq data analyses.

# By gene

## Overview 

The simplest approach is to resample the genes to generate a "replicate" dataset.
Assume that we have a cell-based clustering approach that was used to generate clusterings in the original dataset.
We generate a bootstrap replicate by sampling genes with replacement, apply our clustering method to the replicate, record the clusterings, and repeat.
For two cells that were originally clustered together, we report the percentage of times that they were clustered together in the replicate.
Conversely, for cells that were not originally clustered together, we report the percentage of times that they were _not_ clustered together in the replicate.
This provides a measure of reproducibility for the clustering, with larger values indicating that the clusters are more stable. 

This approach is implemented in a number of software packages such as `r CRANpkg("pvclust")` [@suzuki2006pvclust],
and was probably motivated by the use of bootstrap confidence intervals for phylogenetic trees [@efron1996bootstrap].
It is appealing as the identities of the cells/samples are not altered, allowing a direct comparison of clusterings between the replicate and original datasets.
However, bootstrapping on genes makes a number of assumptions:

- The genes are independent - or specifically, independent conditional on the underlying structure
(as genes supporting the same clustering would always have correlated expression profiles).
This is untrue due to correlations between co-regulated genes _within_ clusters.
- The genes are drawn from the same distribution.
To see why this is not sensible, consider an example with two clusters and 50 genes that change strongly in each direction between clusters.
For some of these genes, the sign of the change will be flipped upon bootstrapping, which is absurd if the bootstrapped sample is to be considered a replicate.

## Simulation

To illustrate, let's have a look at 100 cells that belong to the same cluster.
These cells express strongly co-regulated genes in each of 5 pathways.
However, there is no systematic structure among the 5 pathways.

```{r}
dataGen <- function (npath, perpath, ncells) {
    each.path <- matrix(rnorm(npath*ncells), ncol=ncells)
    gene.exprs <- each.path[rep(seq_len(npath), each=perpath),]
    gene.exprs + rnorm(length(gene.exprs), 0.1)
}
y <- dataGen(npath=5, perpath=100, ncells=100)
```

We apply simple _k_-means to split this into 4 clusters.
This clustering is completely spurious as there is no real structure here.

```{r}
clusterFun <- function(y) {
    cluster <- kmeans(t(y), centers=4)$cluster
    outer(cluster, cluster, "==")
}
together <- clusterFun(y)
```

We perform bootstrapping on the genes to assess cluster stability.
This is done by checking whether the co-clusterings of cells are preserved in the bootstrap replicate.

```{r}
result <- matrix(0, ncol(y), ncol(y))
for (x in 1:100) {
    new.y <- y[sample(nrow(y), nrow(y), replace=TRUE),]
    new.together <- clusterFun(new.y)
    result <- result + as.integer(new.together==together)
}
```

We can compare this to a true replicate of this dataset.
Recall that the pathway expression is unstructured, so we are not under any obligation to re-use the structure in `y`.

```{r}
result2 <- matrix(0, ncol(y), ncol(y))
for (x in 1:100) {
    new.y <- dataGen(npath=5, perpath=100, ncells=100)
    new.together2 <- clusterFun(new.y)
    result2 <- result2 + as.integer(new.together2==together)
}
```

We can see that the reproducibility values are consistently overestimated by bootstrapping.
This is a consequence of the correlations between genes. 
The true reproducibility for co-clustering is close to 25%, as cells are split across 4 clusters.

```{r, fig.wide=TRUE}
par(mfrow=c(1,2))
boxplot(split(result, together), ylab="Reproducibility (%)", 
    xlab="Same cluster", main="Bootstrapping", ylim=c(0, 100))
boxplot(split(result2, together), ylab="Reproducibility (%)", 
    xlab="Same cluster", main="Truth", ylim=c(0, 100))
```

Conversely, bootstrapping is accurate if the genes are actually independent.
This is consistent with the underlying assumptions of the gene-based bootstrap.

```{r, fig.wide=TRUE}
y <- dataGen(npath=500, perpath=1, ncells=100)
together <- clusterFun(y)

result3 <- matrix(0, ncol(y), ncol(y))
for (x in 1:100) {
    new.y <- y[sample(nrow(y), nrow(y), replace=TRUE),]
    new.together <- clusterFun(new.y)
    result3 <- result3 + as.integer(new.together==together)
}

result4 <- matrix(0, ncol(y), ncol(y))
for (x in 1:100) {
    new.y <- dataGen(npath=500, perpath=1, ncells=100)
    new.together <- clusterFun(new.y)
    result4 <- result4 + as.integer(new.together==together)
}

par(mfrow=c(1,2))
boxplot(split(result3, together), ylab="Reproducibility (%)", 
    xlab="Same cluster", main="Bootstrapping", ylim=c(0, 100))
boxplot(split(result4, together), ylab="Reproducibility (%)", 
    xlab="Same cluster", main="Truth", ylim=c(0, 100))
```

More subtle variants of this problem arise when there is actually some weak underlying structure.
In such cases, the correlation-induced bias of the bootstrap estimates may inflate the apparent stability of the clusters.
This defeats the purpose of using these estimates as an objective criterion for determining the number and reliability of clusters.

# By cell

## Overview

The alternative approach to bootstrapping is to resample the cells with replacement.
Here we are resampling observations, which is closer to the original application of the bootstrap [@efron1986bootstrap].
We can reasonably assume that cells were independently^[Depending on the experimental protocol used for sampling the cells, of course.] drawn from the same underlying population,
such that the cell-bootstrapped replicate is a close approximation of a real experimental replicate.
No assumptions about the independence of genes are involved, which is appealing.

It is important to keep in mind that bootstrapping on the cells fundamentally changes the cell identities in the bootstrap replicate.
A resampled cell in a bootstrap replicate cannot be considered to be the same as the cell-of-origin in the original dataset.
This reflects the fact that, in a real scRNA-seq experiment, we cannot obtain replicate observations from the same cell.
The resampled cell is a conceptually new cell, which simply happens to have an expression profile identical to the cell-of-origin. 

The question then becomes, how do we assess cluster stability if the cell identities have changed in the replicate?
The `r CRANpkg("fpc")` package [@hennig2007clusterwise] provides cluster-specific stability measures by bootstrapping on the cells with the `clusterboot` function.
This is achieved by computing the maximum Jaccard similarity between each bootstrapped and original cluster, and taking the mean Jaccard index across bootstrap iterations.
Larger Jaccard indices indicate that the cluster was mostly recovered in the bootstrap replicate.

The `clusterboot` approach relies on a mapping between cells in the original dataset to that in the bootstrap replicate.
This is achieved by keeping track of the identities of the resampled cells in each bootstrap iteration.
At first glance, this seems inappropriate as cell identities are inherently altered by bootstrapping.
However, if we did have a replicate, we could construct a mapping using a nearest-neighbour approach; 
and the nearest neighbour of each resampled cell in the original dataset would simply be the cell-of-origin, as they have the same expression profile!

## Simulation

We can check whether the `clusterboot` approach yields sensible Jaccard indices with a small simulation.
First, we set up a function to map replicate cells to the original dataset, and to calculate the Jaccard indices:

```{r}
library(FNN)
mapJaccard <- function(y, cluster, new.y, new.cluster) {
    m <- get.knnx(t(y), t(new.y), k=1)$nn.index
    re.cluster <- cluster[m]

    by.reclust <- split(seq_along(re.cluster), re.cluster)
    by.newclust <- split(seq_along(new.cluster), new.cluster)
    output <- numeric(length(by.newclust))
    names(output) <- names(by.newclust)

    # Original clusters with no cells in the bootstrap get Jaccard=0.
    for (X in names(by.reclust)) {
        current <- by.reclust[[X]]
        jaccards <- lapply(by.newclust, FUN=function(other) {
            length(intersect(current, other))/length(union(current,other))
        })
        output[X] <- max(unlist(jaccards))
    }
    return(output)
}
```

We also set up a simple _k_-means function to report the clustering:

```{r}
clusterFun2 <- function(y) {
    kmeans(t(y), centers=4)$cluster
}
```

We use bootstrapping to compute Jaccard indices:

```{r}
y <- dataGen(npath=500, perpath=1, ncells=100)
cluster <- clusterFun2(y)

result3 <- vector("list", 100)
for (x in seq_along(result3)) {
    new.y <- y[sample(nrow(y), nrow(y), replace=TRUE),]
    new.cluster <- clusterFun2(new.y)
    result3[[x]] <- mapJaccard(y, cluster, new.y, new.cluster)
}
result3 <- do.call(rbind, result3)
colMeans(result3)
```

... and we repeat this with true replicates.
The two sets of Jaccard indices are quite similar, which suggests that the implicit mapping used by `clusterboot` is acceptable
(and that the variability of the nearest-neighbour mapping can be ignored).

```{r}
result4 <- vector("list", 100)
for (x in seq_along(result4)) { 
    new.y <- dataGen(npath=500, perpath=1, ncells=100)
    new.cluster <- clusterFun2(new.y)
    result4[[x]] <- mapJaccard(y, cluster, new.y, new.cluster)
}
result4 <- do.call(rbind, result4)
colMeans(result4)
```

# Interpreting the cluster stability

## Difficulties

The real problem with `clusterboot` is the interpretability of the Jaccard indices.
A Jaccard index of 0.9 may be very good for cell types that are difficult to separate, e.g., various flavours of T cells.
In contrast, any Jaccard index below 1 would probably be unacceptable for very distinct cell types, e.g., mESCs and MEFs.

Similar issues are present with metrics based on the number of times two cells are observed in the same cluster.
This describes the relationship between pairs of cells, and relating this to the stability of an entire cluster is not straightforward.
Yes, we _could_ compute any number of summary statistics, but this provides no real advantage in interpretability over the Jaccard index.

Another issue is that cluster stability becomes contextual as soon as multiple clusters are involved.
A cluster may not be stable with respect to separation from a neighbouring cluster, but may be very consistently separated from other distant clusters.
Strictly speaking, it is true that such a cluster is unstable, but this is only relevant when considering the differences between the adjacent clusters.
Interpreting a single stability metric per cluster is difficult as the reason for any low stability is not obvious (and may or may not be important).

## Potential solution

We propose a more interpretable metric with the following procedure:

1. We bootstrap on the cells, recluster and obtain a new set of clusters.
2. We assign each original cluster to the closest new cluster based on their mean expression profiles.
3. For each _pair_ of original clusters, we count the number of bootstrap iterations in which they are assigned to the same new cluster.
4. A high frequency of co-assignments indicates that the paired clusters are not stable with respect to each other.

The co-assignment frequency can be interpreted as the probability of merging two clusters, 
using the nearest-neighbour mapping to determine which clusters are "equivalent" across replicates.
High probabilities indicate that the two clusters are not stable _with respect to each other_.
This is much more useful as it allows users to evaluate the reliability of separation for different clusters.
For example, we could ignore any separation of cluster pairs that are merged more than 5% of the time.

An interesting case involves that of rare cell populations, which may not even be sampled in the boostrap replicate.
This will result in a high co-assignment frequency as the closest new cluster will frequently correspond to another original cluster.
As far as we are concerned, this is the correct result as the rare cell population cannot be consistently recovered in the bootstrap replicates.
The closest original cluster will also have a high co-assignment frequency but only to the rare population, so interpretation of other separations is not compromised.

# Does this even make sense?

Bootstrapping aims to determine the stability of the clusters in a hypothetical replicate.
We would expect stability to increase with the number of cells as sampling noise becomes negligible;
at some point, every cluster is probably stable according to bootstrapping, 
which is technically correct but not helpful for determining whether clusters are reliable.
Indeed, there is a large space between "this cluster separation is stable" and "this cluster separation is interesting",
and bootstrapping only evaluates the former.

Another problem is that bootstrapping fails to capture the true variability across samples.
A bootstrap replicate is not a good representative of a genuine replicate generated from a different set of cells (most likely with different cell type composition and changes in expression).
This means that bootstrapping will be overly optimistic with respect to what it considers to be reproducible.

Conversely, we might actually consider the comparisons between clusters to be too strict.
The clustering algorithm may merge or split apart clusters due to the noise introduced by bootstrapping, 
but it is often still possible to effectively recover the same clusters by tweaking the parameters for the boostrap replicate.
In such cases, the scientific conclusions are reproducible across bootstrap replicates but bootstrap-based stability measures would be overly pessimistic.

# Session information

```{r}
sessionInfo()
```

# References

