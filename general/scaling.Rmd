# Should we standardize the log-counts?

## Rationale

Standardization ensures that all genes contribute the same amount of variance to downstream steps like PCA and clustering.
This avoids problems caused by the mean-variance relationship of log-count data.
In particular, it means that genes in the middle of the range with high technical variance will not dominate the variance of the data set.

## Drawbacks of standardization

Standardizing will downweight the contribution of interesting genes with large total variances due to biological heterogeneity. 
This will reduce the resolution of biological differences between cell populations.
Of course, some genes may just be biologically noisy, but more often than not, large biological components will represent some interesting structured variation.
Conversely, standardization will implicitly upweight genes with low total variance and small (non-zero) biological components.
This will amplify biological variability that was originally minor, which may be misleading.

```{r}
set.seed(10)
a <- matrix(rnorm(100000), ncol=100)
a[1:10,1:50] <- a[1:10,1:50] + 10

out.raw <- prcomp(t(a))
out.sca <- prcomp(t(a), scale=TRUE)

# Populations are less clearly separated after scaling.
col <- rep(c("blue", "red"), each=50)
plot(out.raw$x[,1], out.raw$x[,2], col=col, main="Raw")
plot(out.sca$x[,1], out.sca$x[,2], col=col, main="Scaled")
```

The effects of standardization are also unpredictable, as distances between subpopulations become dependent on other structures in the data.
To illustrate, imagine a dataset containing two subpopulations.
One of them highly expresses gene X, while the other only has moderate expression - thus, we are able to distinguish these two populations on the expression of gene X.
Now, imagine adding a third subpopulation that is silent for gene X.
If standardization is performed, this will reduce the power of X to discriminate between the first two subpopulations.
This is counterintuitive as nothing has changed between the first two subpopulations.

Any scaling distorts the true log-fold changes for genes between subpopulations, though I would hope that the scaled data are not being used for DE analyses.
It also affects interpretation of relative distances between three or more groups of cells, by changing the scale of the log-fold changes across genes.
That is, are two groups more related than a third group?
However, this is probably less relevant as log-fold changes between different genes are difficult to compare sensibly anyway.

## Exploiting the technical component

The main aim of standardization is to eliminate uninteresting differences in variance between genes.
Unfortunately, we can't say whether the biological component is interesting or not from its magnitude alone.
If we only wanted the structured heterogeneity, we'd only be able to make some kind of distinction after clustering.
(In which case, scaling based on the clusters and then re-clustering would probably pose some problems with circularity.)

The only aspect that we can confidently say is uninteresting is the technical component, e.g., due to sequencing/capture/amplification noise in the protocol.
In theory, this is a better justification for standardization.
Most genes will have low-to-zero biological components, which suggests that standardization will equalize most of the technical components across the dataset.
This means that we can avoid domination by a few genes with large technical components.

The question is whether the decreases in the technical components are faster than the decreases in the biological components.
This is unlikely, given that any non-zero biological component will increase the total variance and result in faster down-scaling for the corresponding gene.
However, it is possible in some situations, e.g., when high-abundance genes are enriched for large biological components.
In such cases, the technically variable genes in the middle of the range would be downweighted with relatively little harm to the biological component.

## Bias versus precision

The decision to use standardization represents a bias-precision trade-off for the estimate of the underlying substructure in the data.
Standardization will bias the estimate in favour of (potentially!) improving precision by reducing the contribution of genes with large technical components.
Personally, I don't think that the presence of a few genes with large technical components is a major concern.
With thousands of genes in PCA or clustering, one should still be able to recover the substructure, regardless of the random scatter in each dimension.

## Alternative strategies

A more refined option is to scale each gene such that its variance becomes equal to the biological component.
This accounts for the mean-variance trend and upweights genes with large biological components (rather than penalizing them).
Conversely, genes with near-zero biological components are effectively ignored during the PCA.
However, this is still an _ad hoc_ strategy.
For total variance $V$ decomposed into $B + T$, the rescaled biological component becomes $B^2/V$ while the rescaled technical component is $TB/V$;
neither of these values has much meaning to me, and treating them as $B$ and $T$ would clearly be wrong unless $T = 0$.
