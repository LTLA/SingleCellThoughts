# Should we standardize the log-counts?

## Rationale

Standardization ensures that all genes contribute the same amount of variance to downstream steps like PCA and clustering.
This avoids problems caused by the mean-variance relationship of log-count data.
In particular, it means that genes in the middle of the range with high technical variance will not dominate the variance of the data set.

## Drawbacks of standardization

Standardizing will downweight the contribution of interesting genes with large total variances due to biological heterogeneity. 
This will reduce the resolution of biological differences between cell populations.
Of course, some genes may just be biologically noisy, but more often than not, large biological components will represent some interesting structured variation.
Conversely, standardization will implicitly upweight genes with low total variance and small (non-zero) biological components.
This will amplify biological variability that was originally minor, which may be misleading.

```{r}
set.seed(10)
a <- matrix(rnorm(100000), ncol=100)
a[1:10,1:50] <- a[1:10,1:50] + 10

out.raw <- prcomp(t(a))
out.sca <- prcomp(t(a), scale=TRUE)

# Populations are less clearly separated after scaling.
col <- rep(c("blue", "red"), each=50)
plot(out.raw$x[,1], out.raw$x[,2], col=col, main="Raw")
plot(out.sca$x[,1], out.sca$x[,2], col=col, main="Scaled")
```

The effects of standardization are also unpredictable, as distances between subpopulations become dependent on other structures in the data.
To illustrate, imagine a dataset containing two subpopulations.
One of them highly expresses gene X, while the other only has moderate expression - thus, we are able to distinguish these two populations on the expression of gene X.
Now, imagine adding a third subpopulation that is silent for gene X.
If standardization is performed, this will reduce the power of X to discriminate between the first two subpopulations.
This is counterintuitive as nothing has changed between the first two subpopulations.

Any scaling distorts the true log-fold changes for genes between subpopulations.
This affects interpretation of relative distances between three or more groups of cells, i.e., are two groups more related than a third group?
Arguably, comparisons of log-fold changes between genes are difficult to interpret, 
but we do so implicitly when computing distances between cells based on transcriptional profiles. 
Standardization will not yield faithful estimates of the log-fold changes, which complicates interpretation even further.

## Exploiting the technical component

The main aim of standardization is to eliminate uninteresting differences in variance between genes.
Unfortunately, we can't say whether the biological component is interesting or not from its magnitude alone.
If we only wanted the structured heterogeneity, we'd only be able to make some kind of distinction after clustering.
(In which case, scaling based on the clusters and then re-clustering would probably pose some problems with circularity.)

The only aspect that we can confidently say is uninteresting is the technical component, e.g., due to sequencing/capture/amplification noise in the protocol.
In theory, this is a better justification for standardization.
Most genes will have low-to-zero biological components, which suggests that standardization will equalize most of the technical components across the dataset.
This means that we can avoid domination by a few genes with large technical components.

The first question is whether the decrease in the total technical component due to standardization is faster than the decrease in the total biological component.
This is generally unlikely as any non-zero biological component will increase the total variance, leading to faster down-scaling for the corresponding gene.
Any positive outcome would require some interaction with the strong mean-variance trend in log-count data.
For example, if the noisiest genes in the mid-abundance range do not contain any biological structure, downscaling their variance would be beneficial.

The second question is how standardization affects the relative contribution of genes of different abundance.
Genes that have larger technical components will be downscaled more severely, even if they have the same biological component as other genes.
This effectively downweights the mid-abundance genes that have fairly large technical components,
contradicting the argument that standardization helps to cancel out the mean-variance relationship.

## Bias versus precision

The decision to use standardization represents a bias-precision trade-off for the estimate of the underlying substructure in the data.
Standardization will bias the estimate in favour of (potentially!) improving precision by reducing the contribution of genes with large technical components.
Personally, I don't think that the presence of a few genes with large technical components is a major concern.
With thousands of genes in PCA or clustering, one should still be able to recover the substructure, regardless of the random scatter in each dimension.

## Alternative strategies

A more refined option is to scale each gene such that its variance becomes equal to the biological component.
This accounts for the mean-variance trend and upweights genes with large biological components (rather than penalizing them).
Conversely, genes with near-zero biological components are effectively ignored during the PCA.
However, this is still an _ad hoc_ strategy.
For total variance $V$ decomposed into $B + T$, the rescaled biological component becomes $B^2/V$ while the rescaled technical component is $TB/V$;
neither of these values has much meaning to me, and treating them as $B$ and $T$ would clearly be wrong unless $T = 0$.

## Comments on using the raw counts

Standardization seems to be necessary when performing PCA or clustering on the counts rather than log-counts.
Otherwise, the results will be driven primarily by high-abundance genes with the greatest unscaled variance.
Putting aside the problems mentioned above, the use of standardized counts has a number of additional problems.
This is because standardization does not change the mean-variance relationship _within_ a gene.
I have listed a few inter-related issues below:

- Limited dynamic range.
Variation in the data are dominated by absolute differences in large counts.
For example, in a mixture of three populations with mean counts of 1, 10 and 100, the first two are effectively indistinguishable.
- Misrepresentation of variability for large counts.
In the above example, we would conclude that the last population is the most variable on the count scale, e.g., assuming a Poisson distribution.
However, by any other metric (e.g., CV^2^), the last population should be the least variable, especially as sequencing noise ceases to be a factor.
- Absolute differences in counts are uninterpretable.
Computing distances between (standardized) counts is less informative, as the interpretation of the difference depends on the size of the counts.
This cannot be easily incorporated _within_ a gene; the best is to use the average abundance of the gene.






