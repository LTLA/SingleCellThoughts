---
title: Comments on the quality control steps
author: Aaron Lun
date: "`r Sys.Date()`"
output:
  BiocStyle::html_document:
    toc: true
    toc_float: true
---

# Improving resolution with log-transformed QC metrics

The log-transformation improves resolution by compressing large values, typically reducing the MAD relative to the median for distributions with long right tails.
This ensures that "3 MADs away from the median" does not lie beyond all data values.
It also expands the range of small values, making it easier to distinguish between outliers and the edge of the distribution of acceptable values.

From a theoretical perspective, the log-transformation tends to make positive distributions (usually with long right tails) more normal-looking.
The 3-MAD threshold has a fairly simple interpretation for normal distributions, enabling the removal of points with lower than 1% likelihood.

On a more conceptual note, the MAD is necessary to account for genuine biological heterogeneity in these metrics.
That's why we don't use a hard-and-fast fold-change threshold from the median, as this would be too aggressive or not aggressive enough in some situations.

# Interpreting the proportion mapped to spike-ins

It shouldn't matter too much if it's the proportion against total counts, or the ratio relative to the endogenous counts.
This is because we're not measuring an increase in mitochondrial/spike-in counts, but rather, a depletion of endogenous RNA.
If endogenous RNA decreases in low-quality cells, the mitochondrial/spike-in proportions against the total count should both increase.
We don't have to worry about effects of, e.g., an increase in mitochondrial counts affecting the proportion of spike-in counts.

The absolute value of the spike-in proportion can also be used for QC.
You would want about 5-10% of the reads going to the spike-ins.
If this is not the case, it suggests that you need to alter the dilution.
You can also compare the observed proportions to the expected values, which can be calculated if RNA quantification was done on the cells beforehand.
Neither of these approaches provide a threshold for filtering, but they do tell you if the experiment went well or not.

Also, we don't use the logit transform for the proportions, even though on the raw scale we could theoretically end up with a above-unity threshold.
This is because the logit transform compresses changes within the middle of the [0,1] range.
This reduces the resolution for where the threshold would usually be.

# Mitochondrial proportions vs spike-ins

I've generally considered the mitochondrial proportion to be worse than the spike-in proportion for detecting broken cells.
This is because the former is additionally affected by the number and activity of the mitochondria, whereas the latter is not.
The number of mitochondria definitely varies between cell types, see https://dx.doi.org/10.1002/jcp.1041360316;
and the ratio of mitochondrial RNA to total RNA also varies, see Figure 2F of https://dx.doi.org/10.1016/j.cell.2011.06.051.
The proportion seems to go above the 5% threshold that people regularly use, which makes a hard threshold ineffective.

More generally, the use of the mitochondrial proportions depends on a fairly restricted type of damage.
The cell has to leak large amounts of cytoplasmic RNA without losing mitochondria.
In cases of severe damage, only the nucleus would be left, resulting in mitochondrial proportions of zero.
Thus, the spike-in proportion is a more reliable metric for detecting cell damage.

On the flip side, if the number of mitochondria per cell volume were constant (probably untrue), then the mitochondrial proportion would be a better measure of cell damage,
as the spike-in proportion would also be affected by the total RNA content of the cell.
Cells with high mitochondrial proportions also tend to have low total counts in real data, so perhaps the biology is not so pronounced in this metric.
There's also no choice for data without spike-ins, such as droplet data.
In those cases, plotting mitochondrial proportions against the total count can verify that you're not removing obvious biology.

I would only trust mitochondrial proportions if they aligned with the other metrics anyway.
It's too early in the analysis to add a whole bunch of assumptions about consistency of the mitochondrial ratios across the population.
If mitochondrial proportions do vary, the mitochondrial genes should show up as highly variable, at which point you can make a decision about whether to filter on this metric.
You can't do the same with spike-ins as spike-in-specific normalization means that you should never see the spike-ins systematically appear in the HVG set.

# Batch-by-batch quality control

Systematic differences in QC metrics can be handled to some extent using the `batch` argument in the `isOutlier` function.
This is obviously useful for batch effects caused by known differences in experimental processing, e.g., sequencing at different depth or had different amounts of spike-in added.
It may also be useful if an _a priori_ cell type has systematically fewer expressed genes or lower RNA content.
Analyzing all cell types together would inflate the MAD and compromise QC at best, or lead to the entire loss of one cell type at worst.

In general, it seems better to block on more factors rather than fewer, to avoid MAD inflation and improve outlier identification.
The risk is that you'll lose more cells within each batch because the MAD is smaller, such that an uncommon subset of (high-quality) cells end up being removed.
However, this is arguably a different problem, i.e., violation of the homogeneity assumption.
Artificially inflating the MAD to compensate is disingenuous, and one would prefer explicit increasing to `nmads` or just not using the filter altogether.

# Arguments for light-touch (or no) QC

As previously discussed, there's an implicit assumption that these technical metrics are homogeneous across cell subtypes and states.
Some heterogeneity is tolerated provided that the inter-state variance of the metric is smaller than the intra-state variance.
However, this assumption won't be true for extreme cases like erythrocytes, resulting in the incorrect removal of cell types.

I'm not sure there's a good automatic way to distinguish between low quality cells and those from a different cell type when heterogeneous QC metrics are observed.
Poor quality affects the observed expression in ways that are difficult to normalize (see the Illicic paper).
One could argue that it would be better not to do any QC at all to avoid discarding distinct cell types.
However, this has a few problems:

- Low-quality cells can drive selection of HVGs and dominate PC definitions.
This is due to a combination of low precision when the count is small _and_ extreme normalization with small size factors.
In the worst case, the first few HVGs or PCs are dedicated to separating low-quality cells from the others (e.g.,  one PC per cell).
Feature selection would then exclude the actual biology and enrich for technical noise.
Of course, one could counter this by not performing feature selection, but such an approach would degrade performance of all downstream methods.
- Low-quality cells can form their own clusters, complicating downstream interpretation.
For example, all the empty wells might cluster together due to the ambient RNA, even though this is clearly not a cell type.
Alternatively, if mRNA loss is not uniform, damaged cells will have their own "damage profile" corresponding to nuclear-enriched mRNAs and cluster together accordingly.
The formation of a cluster does not necessarily indicate that the cells are of high quality, due to these systematic effects associated with low quality.
- Clusters of low-quality cells can appear to have strong DE compared to other clusters.
Part of this is obviously due to the fact that they have zero counts for many genes.
However, their small size factors can also drive apparent upregulation of the remaining genes (e.g., from the ambient RNA or preferentially retained in the nucleus).
This manifests as misleading marker genes, further complicating assignment of cluster identity.

Thus, the best approach is probably to filter out low-quality cells on the first pass.
This yields a set of reliable results that are generally satisfactory and will take some time to interpret anyway.
Only if there is evidence for discarded cell types (e.g., by making an MA plot of the discarded versus retained expression profiles) should the filters be relaxed.
It is true that we bias against cell types with low RNA content, but this is just a reflection of the overall bias in the scRNA-seq protocol.
The more delicate cell types are likely being lost during dissociation anyway, so the technique was never a truly unbiased assay to start with.

# Cell filtering before gene filtering

It is intuitively logical to remove low-quality cells before computing the average counts and performing gene filtering.
Otherwise, the averages would be distorted by empty cells or cells with 100% spike-ins.
However, one _could_ argue that some cells become low quality after filtering on the genes.
In particular, cells might drop below the "number of expressed features" threshold after some genes are removed.
This would indicate a need for repeated cycles of filtering on cells and genes, which would be rather laborious.

In practice, this is unlikely to be a problem.
Removal of an inordinate number of expressed features in some cells relative to other cells would only occur due to DE genes.
If most genes are not DE, they should be filtered at an equal rate for all cells.
This suggests that filtering is unlikely to cause cells to suddenly become outliers with respect to the number of expressed features.
Most cells surviving the initial round of QC are also likely to have diverse transcriptomes, so the total count should not suddenly drop either.
(In any case, genes that are dominating the sequencing output of a cell would probably not get filtered.)


